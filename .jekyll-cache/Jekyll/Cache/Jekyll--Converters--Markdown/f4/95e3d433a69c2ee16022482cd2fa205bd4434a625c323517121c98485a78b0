I"∫<!-- MarkdownTOC -->

<ul>
  <li><a href="#-gioi-thieu">1. Gi·ªõi thi·ªáu</a></li>
  <li><a href="#-co-so-toan-hoc">2. C∆° s·ªü to√°n h·ªçc</a></li>
  <li><a href="#-ham-so-kernel">3. H√†m s·ªë kernel</a>
    <ul>
      <li><a href="#-tinh-chat-cua-cac-ham-kerrnel">3.1. T√≠nh ch·∫•t c·ªßa c√°c h√†m kerrnel</a></li>
      <li><a href="#-mot-so-ham-kernel-thong-dung">3.2. M·ªôt s·ªë h√†m kernel th√¥ng d·ª•ng</a>
        <ul>
          <li><a href="#-linear">3.2.1. Linear</a></li>
          <li><a href="#-polynomial">3.2.2. Polynomial</a></li>
          <li><a href="#-radial-basic-function">3.2.3. Radial Basic Function</a></li>
          <li><a href="#-sigmoid">3.2.4. Sigmoid</a></li>
          <li><a href="#-bang-tom-tat-cac-kernel-thong-dung">3.2.5. B·∫£ng t√≥m t·∫Øt c√°c kernel th√¥ng d·ª•ng</a></li>
          <li><a href="#-kernel-tu-dinh-nghia">3.2.6. Kernel t·ª± ƒë·ªãnh nghƒ©a</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#-vi-du-minh-hoa">4. V√≠ d·ª• minh h·ªça</a>
    <ul>
      <li><a href="#-bai-toan-xor">4.1. B√†i to√°n XOR</a></li>
      <li><a href="#-du-lieu-gan-linearly-separable">4.2. D·ªØ li·ªáu g·∫ßn linearly separable</a></li>
      <li><a href="#-bai-toan-phan-biet-gioi-tinh">4.3. B√†i to√°n ph√¢n bi·ªát gi·ªõi t√≠nh</a></li>
    </ul>
  </li>
  <li><a href="#-tom-tat">5. T√≥m t·∫Øt</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. T√†i li·ªáu tham kh·∫£o</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><em>B·∫°n ƒë·ªçc ƒë∆∞·ª£c khuy·∫øn kh√≠ch ƒë·ªçc <a href="https://machinelearningcoban.com/2017/04/09/smv/">B√†i 19</a> v√† <a href="https://machinelearningcoban.com/2017/04/13/softmarginsmv/">B√†i 20</a> tr∆∞·ªõc khi ƒë·ªçc b√†i n√†y.</em>
<a name="-gioi-thieu"></a></p>

<h2 id="1-gi·ªõi-thi·ªáu">1. Gi·ªõi thi·ªáu</h2>

<p>C√≥ m·ªôt s·ª± t∆∞∆°ng ·ª©ng th√∫ v·ªã gi·ªØa hai nh√≥m thu·∫≠t to√°n ph√¢n l·ªõp ph·ªï bi·∫øn nh·∫•t: Neural Network v√† Support Vector Machine. Ch√∫ng ƒë·ªÅu b·∫Øt ƒë·∫ßu t·ª´ b√†i to√°n ph√¢n l·ªõp v·ªõi 2 <em>linearly separable classes</em>, ti·∫øp theo ƒë·∫øn 2 <em>almost linear separable classes</em>, ƒë·∫øn b√†i to√°n c√≥ nhi·ªÅu classes r·ªìi c√°c b√†i to√°n v·ªõi bi√™n kh√¥ng tuy·∫øn t√≠nh. S·ª± t∆∞∆°ng ·ª©ng ƒë∆∞·ª£c cho trong b·∫£ng d∆∞·ªõi ƒë√¢y:</p>

<hr />

<table>
  <thead>
    <tr>
      <th>Neural Networks</th>
      <th>Support Vector Machine</th>
      <th>T√≠nh ch·∫•t chung</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://machinelearningcoban.com/2017/01/21/perceptron/">PLA</a></td>
      <td><a href="https://machinelearningcoban.com/2017/04/09/smv/">Hard Margin SVM</a></td>
      <td>Hai classes l√† <em>linearly separable</em></td>
    </tr>
    <tr>
      <td><a href="https://machinelearningcoban.com/2017/01/27/logisticregression/">Logistic Regression</a></td>
      <td><a href="https://machinelearningcoban.com/2017/04/13/softmarginsmv/">Soft Margin SVM</a></td>
      <td>Hai classes l√† <em>g·∫ßn linearly separable</em></td>
    </tr>
    <tr>
      <td><a href="https://machinelearningcoban.com/2017/02/17/softmax/">Softmax Regression</a></td>
      <td>Multi-class SVM</td>
      <td>B√†i to√°n ph√¢n lo·∫°i nhi·ªÅu classes (bi√™n l√† tuy·∫øn t√≠nh)</td>
    </tr>
    <tr>
      <td><a href="https://machinelearningcoban.com/2017/02/24/mlp/">Multi-layer Perceptron</a></td>
      <td>Kernel SVM</td>
      <td>B√†i to√°n v·ªõi d·ªØ li·ªáu kh√¥ng <em>linearly separable</em></td>
    </tr>
  </tbody>
</table>

<hr />

<p>Trong B√†i 21 n√†y, t√¥i s·∫Ω vi·∫øt v·ªÅ Kernel SVM, t·ª©c vi·ªác √°p d·ª•ng SVM l√™n b√†i to√°n m√† d·ªØ li·ªáu gi·ªØa hai classes l√† ho√†n to√†n <em>kh√¥ng linear separable</em> (t√¥i t·∫°m d·ªãch l√† <em>kh√¥ng ph√¢n bi·ªát tuy·∫øn t√≠nh</em>). B√†i to√°n ph√¢n bi·ªát nhi·ªÅu classes s·∫Ω ƒë∆∞·ª£c t√¥i tr√¨nh b√†y trong B√†i 22: Multiclass SVM.</p>

<p>√ù t∆∞·ªüng c∆° b·∫£n c·ªßa Kernel SVM v√† c√°c ph∆∞∆°ng ph√°p kernel n√≥i chung l√† t√¨m m·ªôt ph√©p bi·∫øn ƒë·ªïi sao cho d·ªØ li·ªáu ban ƒë·∫ßu l√† <em>kh√¥ng ph√¢n bi·ªát tuy·∫øn t√≠nh</em> ƒë∆∞·ª£c bi·∫øn sang kh√¥ng gian m·ªõi. ·ªû kh√¥ng gian m·ªõi n√†y, d·ªØ li·ªáu tr·ªü n√™n <em>ph√¢n bi·ªát tuy·∫øn t√≠nh</em>.</p>

<p>X√©t v√≠ d·ª• d∆∞·ªõi ƒë√¢y v·ªõi vi·ªác bi·∫øn d·ªØ li·ªáu <em>kh√¥ng ph√¢n bi·ªát tuy·∫øn t√≠nh</em> trong kh√¥ng gian hai chi·ªÅu th√†nh <em>ph√¢n bi·ªát tuy·∫øn t√≠nh</em> trong kh√¥ng gian ba chi·ªÅu b·∫±ng c√°ch gi·ªõi thi·ªáu th√™m m·ªôt chi·ªÅu m·ªõi:</p>

<hr />

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/5.png" />
         <br />
        a)
         </td>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/4.png" />
         <br />
        b)
        </td>

    </tr>
    <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/6.png" />
         <br />
        c)
         </td>
        <td width="40%" style="border: 0px solid white" align="justify">
        H√¨nh 1: V√≠ d·ª• v·ªÅ Kernel SVM. a) D·ªØ li·ªáu c·ªßa hai classes l√† <em>kh√¥ng ph√¢n bi·ªát tuy·∫øn t√≠nh</em> trong kh√¥ng gian hai chi·ªÅu. b) N·∫øu coi th√™m chi·ªÅu th·ª© ba l√† m·ªôt h√†m s·ªë c·ªßa hai chi·ªÅu c√≤n l·∫°i \(z = x^2 + y^2\), c√°c ƒëi·ªÉm d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c ph√¢n b·ªë tr√™n 1 parabolic v√† ƒë√£ tr·ªü n√™n <em>ph√¢n bi·ªát tuy·∫øn t√≠nh </em>. M·∫∑t ph·∫≥ng m√†u v√†ng l√† m·∫∑t ph√¢n chia, c√≥ th·ªÉ t√¨m ƒë∆∞·ª£c b·ªüi Hard/Soft Margin SVM. c) Giao ƒëi·ªÉm c·ªßa m·∫∑t ph·∫≥ng t√¨m ƒë∆∞·ª£c v√† m·∫∑t parabolic l√† m·ªôt ƒë∆∞·ªùng ellipse, khi chi·∫øu to√†n b·ªô d·ªØ li·ªáu c≈©ng nh∆∞ ƒë∆∞·ªùng ellipse n√†y xu·ªëng kh√¥ng gian hai chi·ªÅu ban ƒë·∫ßu, ta ƒë√£ t√¨m ƒë∆∞·ª£c ƒë∆∞·ªùng ph√¢n chia hai classses.
        </td>
    </tr>
</table>
</div>
<hr />

<p>ƒê·ªÉ xem v√≠ d·ª• n√†y m·ªôt c√°ch sinh ƒë·ªông h∆°n, b·∫°n c√≥ th·ªÉ xem clip nh·ªè d∆∞·ªõi ƒë√¢y:</p>

<div style="text-align:center;">
<iframe width="600" height="400" src="https://www.youtube.com/embed/04eOsL5vrWc" frameborder="0" allowfullscreen=""></iframe>
<div class="thecap">M·ªôt v√≠ d·ª• v·ªÅ ph∆∞∆°ng ph√°p kernel.</div>
</div>

<p>N√≥i m·ªôt c√°ch ng·∫Øn g·ªçn, Kernel SVM l√† vi·ªác ƒëi t√¨m m·ªôt h√†m s·ªë bi·∫øn ƒë·ªïi d·ªØ li·ªáu \(\mathbf{x}\) t·ª´ kh√¥ng gian <em>feature</em> ban ƒë·∫ßu th√†nh d·ªØ li·ªáu trong m·ªôt kh√¥ng gian m·ªõi b·∫±ng h√†m s·ªë \(\Phi(\mathbf{x})\). Trong v√≠ d·ª• n√†y, h√†m \(\Phi()\) ƒë∆°n gi·∫£n l√† gi·ªõi thi·ªáu th√™m m·ªôt chi·ªÅu d·ªØ li·ªáu m·ªõi (m·ªôt feature m·ªõi) l√† m·ªôt h√†m s·ªë c·ªßa c√°c <em>features</em> ƒë√£ bi·∫øt. H√†m s·ªë n√†y c·∫ßn th·ªèa m√£n m·ª•c ƒë√≠ch c·ªßa ch√∫ng ta: trong kh√¥ng gian m·ªõi, d·ªØ li·ªáu gi·ªØa hai classes l√† <em>ph√¢n bi·ªát tuy·∫øn t√≠nh</em> ho·∫∑c <em>g·∫ßn nh∆∞ ph·∫ßn bi·ªát tuy·∫øn t√≠nh</em>. Khi ƒë√≥, ta c√≥ th·ªÉ d√πng c√°c b·ªô ph√¢n l·ªõp tuy·∫øn t√≠nh th√¥ng th∆∞·ªùng nh∆∞ PLA, Logistic Regression, hay Hard/Soft Margin SVM.</p>

<p>N·∫øu ph·∫£i so s√°nh, ta c√≥ th·ªÉ th·∫•y r·∫±ng h√†m bi·∫øn ƒë·ªïi \(\Phi()\) t∆∞∆°ng t·ª± nh∆∞ <a href="/2017/02/24/mlp/#-activation-functions"><em>activation functions</em></a> trong Neural Networks. Tuy nhi√™n, c√≥ m·ªôt ƒëi·ªÉm kh√°c bi·ªát ·ªü ƒë√¢y l√†: trong khi nhi·ªám v·ª• c·ªßa activation function l√† ph√° v·ª° t√≠nh tuy·∫øn t√≠nh c·ªßa <em>m√¥ h√¨nh</em>, h√†m bi·∫øn ƒë·ªïi \(\Phi()\) ƒëi bi·∫øn <em>d·ªØ li·ªáu</em> kh√¥ng ph√¢n bi·ªát tuy·∫øn t√≠nh th√†nh ph√¢n bi·ªát tuy·∫øn t√≠nh. Nh∆∞ v·∫≠y l√† ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ª•c ƒë√≠ch chung, ta c√≥ hai c√°ch nh√¨n kh√°c nhau v·ªÅ c√°ch gi·∫£i quy·∫øt.</p>

<p>C√°c h√†m \(\Phi()\) th∆∞·ªùng t·∫°o ra d·ªØ li·ªáu m·ªõi c√≥ s·ªë chi·ªÅu cao h∆°n s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu ban ƒë·∫ßu, th·∫≠m ch√≠ l√† v√¥ h·∫°n chi·ªÅu. N·∫øu t√≠nh to√°n c√°c h√†m n√†y tr·ª±c ti·∫øp, ch·∫Øc ch·∫Øn ch√∫ng ta s·∫Ω g·∫∑p c√°c v·∫•n ƒë·ªÅ v·ªÅ b·ªô nh·ªõ v√† hi·ªáu nƒÉng t√≠nh to√°n. C√≥ m·ªôt c√°ch ti·∫øp c·∫≠n l√† s·ª≠ d·ª•ng c√°c <em>kernel functions</em> m√¥ t·∫£ quan h·ªá gi·ªØa hai ƒëi·ªÉm d·ªØ li·ªáu b·∫•t k·ª≥ trong kh√¥ng gian m·ªõi, thay v√¨ ƒëi t√≠nh to√°n tr·ª±c ti·∫øp t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu trong kh√¥ng gian m·ªõi. K·ªπ thu·∫≠t n√†y ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n quan s√°t v·ªÅ <a href="/2017/04/09/smv/#-bai-toan-doi-ngau-cho-svm">b√†i to√°n ƒë·ªëi ng·∫´u c·ªßa SVM</a>.</p>

<p>Trong M·ª•c 2 d∆∞·ªõi ƒë√¢y, ch√∫ng ta c√πng t√¨m hi·ªÉu c∆° s·ªü to√°n h·ªçc c·ªßa Kernel SVM v√† M·ª•c 3 s·∫Ω gi·ªõi thi·ªáu m·ªôt s·ªë h√†m Kernel th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng.</p>

<p><a name="-co-so-toan-hoc"></a></p>

<h2 id="2-c∆°-s·ªü-to√°n-h·ªçc">2. C∆° s·ªü to√°n h·ªçc</h2>
<p>T√¥i xin nh·∫Øc l·∫°i b√†i to√°n ƒë·ªëi ng·∫´u trong Soft Margin SVM cho d·ªØ li·ªáu <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em>:</p>

<p>\begin{eqnarray}
     \lambda &amp;=&amp; \arg \max_{\lambda} \sum_{n=1}^N \lambda_n - \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^N \lambda_n \lambda_m y_n y_m \mathbf{x}_n^T \mathbf{x}_m &amp;&amp;\<br />
     \text{subject to:}~ &amp;&amp; \sum_{n=1}^N \lambda_ny_n = 0 &amp;&amp;\quad\quad\quad\quad(1)\<br />
     &amp;&amp; 0 \leq \lambda_n \leq C, ~\forall n= 1, 2, \dots, N 
 \end{eqnarray}</p>

<p>Trong ƒë√≥:</p>

<ul>
  <li>
    <p>\(N\): s·ªë c·∫∑p ƒëi·ªÉm d·ªØ li·ªáu trong t·∫≠p training.</p>
  </li>
  <li>
    <p>\(\mathbf{x}_n\): feature vector c·ªßa d·ªØ li·ªáu th·ª© \(n\) trong t·∫≠p training.</p>
  </li>
  <li>
    <p>\(y_n\): <em>nh√£n</em> c·ªßa d·ªØ li·ªáu th·ª© \(n\), b·∫±ng 1 ho·∫∑c -1.</p>
  </li>
  <li>
    <p>\(\lambda_n\): nh√¢n t·ª≠ Lagrange ·ª©ng v·ªõi ƒëi·ªÉm d·ªØ li·ªáu th·ª© \(n\).</p>
  </li>
  <li>
    <p>\(C\): h·∫±ng s·ªë d∆∞∆°ng gi√∫p c√¢n ƒë·ªëi ƒë·ªô l·ªõn c·ªßa <em>margin</em> v√† <em>s·ª± hy sinh</em> c·ªßa c√°c ƒëi·ªÉm n·∫±m trong v√πng <em>kh√¥ng an to√†n</em>. Khi \(C = \infty\) ho·∫∑c r·∫•t l·ªõn, Soft Margin SVM tr·ªü th√†nh Hard Margin SVM.</p>
  </li>
</ul>

<p>Sau khi gi·∫£i ƒë∆∞·ª£c \(\lambda\) cho b√†i to√°n \((1)\), <em>nh√£n</em> c·ªßa m·ªôt ƒëi·ªÉm d·ªØ li·ªáu m·ªõi s·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi d·∫•u c·ªßa bi·ªÉu th·ª©c: 
\[
\sum_{m \in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T \mathbf{x} + \frac{1}{N_{\mathcal{M}}} \sum_{n \in \mathcal{M}} \left(y_n - \sum_{m \in \mathcal{S}} \lambda_m y_m \mathbf{x}_m^T\mathbf{x}_n\right)~~~~~~~~~ (2)
\]</p>

<p>Trong ƒë√≥:</p>

<ul>
  <li>
    <p>\(\mathcal{M} = \{n: 0 &lt; \lambda_n &lt; C\}\) l√† t·∫≠p h·ª£p nh·ªØng ƒëi·ªÉm n·∫±m tr√™n margin.</p>
  </li>
  <li>
    <p>\(\mathcal{S} = \{n: 0 &lt; \lambda_n\}\) l√† t·∫≠p h·ª£p c√°c ƒëi·ªÉm support.</p>
  </li>
  <li>
    <p>\(N_{\mathcal{M}}\) l√† s·ªë ph·∫ßn t·ª≠ c·ªßa \(\mathcal{M}\).</p>
  </li>
</ul>

<p>V·ªõi d·ªØ li·ªáu th·ª±c t·∫ø, r·∫•t kh√≥ ƒë·ªÉ c√≥ d·ªØ li·ªáu <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em>, v√¨ v·∫≠y nghi·ªám c·ªßa b√†i to√°n \((1)\) c√≥ th·ªÉ kh√¥ng th·ª±c s·ª± t·∫°o ra m·ªôt b·ªô ph√¢n l·ªõp t·ªët. Gi·∫£ s·ª≠ r·∫±ng ta c√≥ th·ªÉ t√¨m ƒë∆∞·ª£c h√†m s·ªë \(\Phi()\) sao cho sau khi ƒë∆∞·ª£c bi·∫øn ƒë·ªïi sang kh√¥ng gian m·ªõi, m·ªói ƒëi·ªÉm d·ªØ li·ªáu \(\mathbf{x}\) tr·ªü th√†nh \(\Phi(\mathbf{x})\), v√† trong kh√¥ng gian m·ªõi n√†y, d·ªØ li·ªáu tr·ªü n√™n <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em>. L√∫c n√†y, <em>hy v·ªçng r·∫±ng</em> nghi·ªám c·ªßa b√†i to√°n Soft Margin SVM s·∫Ω cho ch√∫ng ta m·ªôt b·ªô ph√¢n l·ªõp t·ªët h∆°n.</p>

<p>Trong kh√¥ng gian m·ªõi, b√†i to√°n \((1)\) tr·ªü th√†nh: 
 \begin{eqnarray}
     \lambda &amp;=&amp; \arg \max_{\lambda} \sum_{n=1}^N \lambda_n - \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^N \lambda_n \lambda_m y_n y_m \Phi(\mathbf{x}_n)^T \Phi(\mathbf{x}_m) &amp;&amp;\<br />
     \text{subject to:}~ &amp;&amp; \sum_{n=1}^N \lambda_ny_n = 0 &amp;&amp;\quad\quad\quad\quad(3)\<br />
     &amp;&amp; 0 \leq \lambda_n \leq C, ~\forall n= 1, 2, \dots, N 
 \end{eqnarray}</p>

<p>v√† <em>nh√£n</em> c·ªßa m·ªôt ƒëi·ªÉm d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi d·∫•u c·ªßa bi·ªÉu th·ª©c:</p>

<p>\[
\mathbf{w}^T\Phi(\mathbf{x}) + b = \sum_{m \in \mathcal{S}} \lambda_m y_m \Phi(\mathbf{x}_m)^T \Phi(\mathbf{x}) + \frac{1}{N_{\mathcal{M}}} \sum_{n \in \mathcal{M}} \left(y_n - \sum_{m \in \mathcal{S}} \lambda_m y_m \Phi(\mathbf{x}_m)^T\Phi(\mathbf{x}_n)\right)~~~~~~~~~ (4)
\]</p>

<p>Nh∆∞ ƒë√£ n√≥i ·ªü tr√™n, vi·ªác t√≠nh to√°n tr·ª±c ti·∫øp \(\Phi(\mathbf{x})\) cho m·ªói ƒëi·ªÉm d·ªØ li·ªáu c√≥ th·ªÉ s·∫Ω t·ªën r·∫•t nhi·ªÅu b·ªô nh·ªõ v√† th·ªùi gian v√¨ s·ªë chi·ªÅu c·ªßa \(\Phi(\mathbf{x})\) th∆∞·ªùng l√† r·∫•t l·ªõn, c√≥ th·ªÉ l√† v√¥ h·∫°n! Th√™m n·ªØa, ƒë·ªÉ t√¨m <em>nh√£n</em> c·ªßa m·ªôt ƒëi·ªÉm d·ªØ li·ªáu m·ªõi \(\mathbf{x}\), ta l·∫°i ph·∫£i t√¨m bi·∫øn ƒë·ªïi c·ªßa n√≥ \(\Phi(\mathbf{x})\) trong kh√¥ng gian m·ªõi r·ªìi l·∫•y t√≠ch v√¥ h∆∞·ªõng c·ªßa n√≥ v·ªõi t·∫•t c·∫£ c√°c \(\Phi(\mathbf{x}_m)\) v·ªõi \(m\) trong t·∫≠p h·ª£p support. ƒê·ªÉ tr√°nh vi·ªác n√†y, ta quan s√°t th·∫•y m·ªôt ƒëi·ªÅu th√∫ v·ªã sau ƒë√¢y.</p>

<p>Trong b√†i to√°n \((3)\) v√† bi·ªÉu th·ª©c \((4)\), ch√∫ng ta kh√¥ng c·∫ßn t√≠nh tr·ª±c ti·∫øp \(\Phi(\mathbf{x})\) cho m·ªçi ƒëi·ªÉm d·ªØ li·ªáu. Ch√∫ng ta ch·ªâ c·∫ßn t√≠nh ƒë∆∞·ª£c \(\Phi(\mathbf{x})^T\Phi(\mathbf{z})\) d·ª±a tr√™n hai ƒëi·ªÉm d·ªØ li·ªáu \(\mathbf{x}, \mathbf{z}\) b·∫•t k·ª≥! K·ªπ thu·∫≠t n√†y c√≤n ƒë∆∞·ª£c g·ªçi l√† <strong>kernel trick</strong>. Nh·ªØng ph∆∞∆°ng ph√°p d·ª±a tr√™n k·ªπ thu·∫≠t n√†y, t·ª©c thay v√¨ tr·ª±c ti·∫øp t√≠nh t·ªça ƒë·ªô c·ªßa m·ªôt ƒëi·ªÉm trong kh√¥ng gian m·ªõi, ta ƒëi t√≠nh t√≠ch v√¥ h∆∞·ªõng gi·ªØa hai ƒëi·ªÉm trong kh√¥ng gian m·ªõi, ƒë∆∞·ª£c g·ªçi chung l√† <strong>kernel method</strong>.</p>

<p>L√∫c n√†y, b·∫±ng c√°ch ƒë·ªãnh nghƒ©a <em>h√†m kernel</em> \(k(\mathbf{x}, \mathbf{z}) = \Phi(\mathbf{x})^T\Phi(\mathbf{z}) \), ta c√≥ th·ªÉ vi·∫øt l·∫°i b√†i to√°n \((3)\) v√† bi·ªÉu th·ª©c \((4)\) nh∆∞ sau:</p>

<p>\begin{eqnarray}
    \lambda &amp;=&amp; \arg \max_{\lambda} \sum_{n=1}^N \lambda_n - \frac{1}{2} \sum_{n=1}^N\sum_{m=1}^N \lambda_n \lambda_m y_n y_m k(\mathbf{x}_n,\mathbf{x}_m) &amp;&amp;\<br />
    \text{subject to:}~ &amp;&amp; \sum_{n=1}^N \lambda_ny_n = 0 &amp;&amp;\quad\quad\quad\quad(5)\<br />
    &amp;&amp; 0 \leq \lambda_n \leq C, ~\forall n= 1, 2, \dots, N &amp;&amp;
\end{eqnarray}
v√†:</p>

<p>\[
\sum_{m \in \mathcal{S}} \lambda_m y_m k(\mathbf{x}_m, \mathbf{x}) + \frac{1}{N_{\mathcal{M}}} \sum_{n \in \mathcal{M}} \left(y_n - \sum_{m \in \mathcal{S}} \lambda_m y_m k(\mathbf{x}_m, \mathbf{x}_n)\right)~~~~~~~~~ (6)
\]</p>

<p><strong>V√≠ d·ª•:</strong> X√©t ph√©p bi·∫øn ƒë·ªïi 1 ƒëi·ªÉm d·ªØ li·ªáu trong kh√¥ng gian hai chi·ªÅu \(\mathbf{x} = [x_1, x_2]^T\) th√†nh m·ªôt ƒëi·ªÉm trong kh√¥ng gian 5 chi·ªÅu \(\Phi(\mathbf{x}) = [1, \sqrt{2} x_1, \sqrt{2} x_2, x_1^2, \sqrt{2} x_1x_2, x_2^2]^T\). Ta c√≥:</p>

<p>\begin{eqnarray}
\Phi(\mathbf{x})^T\Phi(\mathbf{z}) &amp;=&amp; [1, \sqrt{2} x_1, \sqrt{2} x_2, x_1^2, \sqrt{2} x_1x_2, x_2^2] [1, \sqrt{2} z_1, \sqrt{2} z_2, z_1^2, \sqrt{2} z_1z_2, z_2^2]^T \<br />
&amp;=&amp; 1 + 2x_1z_1 + 2x_2z_2 + x_1^2x_2^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2 \<br />
&amp;=&amp; (1 + x_1z_1 + x_2z_2)^2 = (1 + \mathbf{x}^T\mathbf{z})^2 = k(\mathbf{x}, \mathbf{z})
\end{eqnarray}</p>

<p>Trong v√≠ d·ª• n√†y, r√µ r√†ng r·∫±ng vi·ªác t√≠nh to√°n h√†m kernel \(k()\) cho hai ƒëi·ªÉm d·ªØ li·ªáu d·ªÖ d√†ng h∆°n vi·ªác t√≠nh t·ª´ng \(\Phi()\) r·ªìi nh√¢n ch√∫ng v·ªõi nhau.</p>

<p>V·∫≠y nh·ªØng h√†m s·ªë kernel c·∫ßn c√≥ nh·ªØng t√≠nh ch·∫•t g√¨, v√† nh·ªØng h√†m nh∆∞ th·∫ø n√†o ƒë∆∞·ª£c s·ª≠ d·ª•ng trong th·ª±c t·∫ø?
<a name="-ham-so-kernel"></a></p>

<h2 id="3-h√†m-s·ªë-kernel">3. H√†m s·ªë kernel</h2>
<p><a name="-tinh-chat-cua-cac-ham-kerrnel"></a></p>

<h3 id="31-t√≠nh-ch·∫•t-c·ªßa-c√°c-h√†m-kerrnel">3.1. T√≠nh ch·∫•t c·ªßa c√°c h√†m kerrnel</h3>
<p>Kh√¥ng ph·∫£i h√†m \(k()\) b·∫•t k·ª≥ n√†o c≈©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng. C√°c h√†m kerrnel c·∫ßn c√≥ c√°c t√≠nh ch·∫•t:</p>

<ul>
  <li>
    <p>ƒê·ªëi x·ª©ng: \(k(\mathbf{x}, \mathbf{z}) = k(\mathbf{z}, \mathbf{x}) \). ƒêi·ªÅu n√†y d·ªÖ nh·∫≠n ra v√¨ t√≠ch v√¥ h∆∞·ªõng c·ªßa hai vector c√≥ t√≠nh ƒë·ªëi x·ª©ng.</p>
  </li>
  <li>
    <p><em>V·ªÅ l√Ω thuy·∫øt</em>, h√†m kerrnel c·∫ßn th·ªèa m√£n <a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem#Mercer.27s_condition">ƒëi·ªÅu ki·ªán Mercer</a>: 
\[
\sum_{n=1}^N \sum_{m=1}^N k(\mathbf{x}_m, \mathbf{x}_n) c_nc_m \geq 0, ~~ \forall c_i \in \mathbb{R}, i = 1, 2, \dots, N \quad \quad (7)
\]
T√≠nh ch·∫•t n√†y ƒë·ªÉ ƒë·∫£m b·∫£o cho vi·ªác h√†m m·ª•c ti√™u c·ªßa b√†i to√°n ƒë·ªëi ng·∫´u \((5)\) l√† <em>l·ªìi</em>.</p>
  </li>
  <li>
    <p><em>Trong th·ª±c h√†nh</em>, c√≥ m·ªôt v√†i h√†m s·ªë \(k()\) kh√¥ng th·ªèa m√£n ƒëi·ªÅu ki·ªán Merrcer nh∆∞ng v·∫´n cho k·∫øt qu·∫£ ch·∫•p nh·∫≠n ƒë∆∞·ª£c. Nh·ªØng h√†m s·ªë n√†y v·∫´n ƒë∆∞·ª£c g·ªçi l√† kernel. Trong b√†i vi·∫øt n√†y, t√¥i ch·ªâ t·∫≠p trung v√†o c√°c h√†m kernel th√¥ng d·ª•ng v√† c√≥ s·∫µn trong c√°c th∆∞ vi·ªán.</p>
  </li>
</ul>

<p>N·∫øu m·ªôt h√†m kerrnel th·ªèa m√£n ƒëi·ªÅu ki·ªán \((7)\), x√©t \(c_n = y_n \lambda_n\), ta s·∫Ω c√≥: 
\[
\lambda^T \mathbf{K} \lambda = \sum_{n=1}^N \sum_{m=1}^N k(\mathbf{x}_m, \mathbf{x}_n) y_ny_m \lambda_n \lambda_m \geq 0, ~\forall \lambda_n \quad\quad (8)
\]
v·ªõi \(\mathbf{K}\) l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng m√† ph·∫ßn t·ª≠ ·ªü h√†ng th·ª© \(n\) c·ªôt th·ª© \(m\) c·ªßa n√≥ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·ªüi: 
\(
k_{nm} = y_ny_m k(\mathbf{x}_n, \mathbf{x}_m)
\)</p>

<p>T·ª´ \((8)\) ta suy ra \(\mathbf{K}\) l√† m·ªôt ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng. V√¨ v·∫≠y, b√†i to√°n t·ªëi ∆∞u \((5)\) c√≥ r√†ng bu·ªôc l√† l·ªìi v√† h√†m m·ª•c ti√™u l√† m·ªôt h√†m l·ªìi (m·ªôt quadratic form). V√¨ v·∫≠y ch√∫ng ta c√≥ th·ªÉ gi·∫£i quy·∫øt b√†i to√°n n√†y m·ªôt c√°ch hi·ªáu qu·∫£.</p>

<p>Trong b√†i vi·∫øt n√†y, t√¥i s·∫Ω kh√¥ng ƒëi s√¢u v√†o vi·ªác gi·∫£i quy·∫øt b√†i to√°n \((5)\) v√¨ n√≥ ho√†n to√†n t∆∞∆°ng t·ª± nh∆∞ b√†i to√°n ƒë·ªëi ng·∫´u c·ªßa Soft Margin SVM. Thay v√†o ƒë√≥, t√¥i s·∫Ω tr√¨nh b√†y c√°c h√†m kernel th√¥ng d·ª•ng v√† hi·ªáu nƒÉng c·ªßa ch√∫ng trong c√°c b√†i to√°n th·ª±c t·∫ø. Vi·ªác n√†y s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán th√¥ng qua c√°c v√≠ d·ª• v√† c√°ch s·ª≠ d·ª•ng th∆∞ vi·ªán sklearn.</p>

<p><a name="-mot-so-ham-kernel-thong-dung"></a></p>

<h3 id="32-m·ªôt-s·ªë-h√†m-kernel-th√¥ng-d·ª•ng">3.2. M·ªôt s·ªë h√†m kernel th√¥ng d·ª•ng</h3>
<p><a name="-linear"></a></p>

<h4 id="321-linear">3.2.1. Linear</h4>
<p>ƒê√¢y l√† tr∆∞·ªùng h·ª£p ƒë∆°n gi·∫£n v·ªõi kernel ch√≠nh t√≠ch v√¥ h∆∞·ªõng c·ªßa hai vector: 
\[
k(\mathbf{x}, \mathbf{z}) = \mathbf{x}^T\mathbf{z}
\]</p>

<p>H√†m s·ªë n√†y, <a href="/2017/04/09/smv/#-bai-toan-doi-ngau-lagrange">nh∆∞ ƒë√£ ch·ª©ng minh trong B√†i 19</a>, th·ªèa m√£n ƒëi·ªÅu ki·ªán \((7)\).</p>

<p>Khi s·ª≠ d·ª•ng h√†m <code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code>, kernel n√†y ƒë∆∞·ª£c ch·ªçn b·∫±ng c√°ch ƒë·∫∑t <code class="language-plaintext highlighter-rouge">kernel = 'linear'</code></p>

<p><a name="-polynomial"></a></p>

<h4 id="322-polynomial">3.2.2. Polynomial</h4>

<p>\[
k(\mathbf{x}, \mathbf{z}) = (r + \gamma \mathbf{x}^T\mathbf{z})^d
\]</p>

<p>V·ªõi \(d\) l√† m·ªôt s·ªë d∆∞∆°ng ƒë·ªÉ ch·ªâ b·∫≠c c·ªßa ƒëa th·ª©c. \(d\) c√≥ th·ªÉ kh√¥ng l√† s·ªë t·ª± nhi√™n v√¨ m·ª•c ƒë√≠ch ch√≠nh c·ªßa ta kh√¥ng ph·∫£i l√† b·∫≠c c·ªßa ƒëa th·ª©c m√† l√† c√°ch t√≠nh kernel. Polynomial kernel c√≥ th·ªÉ d√πng ƒë·ªÉ m√¥ t·∫£ h·∫ßu h·∫øt c√°c ƒëa th·ª©c c√≥ b·∫≠c kh√¥ng v∆∞·ª£t qu√° \(d\) n·∫øu \(d\) l√† m·ªôt s·ªë t·ª± nhi√™n.</p>

<p>Ph·∫ßn ki·ªÉm tra li·ªáu h√†m n√†y c√≥ th·ªèa m√£n ƒëi·ªÅu ki·ªán \((7)\) hay kh√¥ng xin ƒë∆∞·ª£c b·ªè qua.</p>

<p>Khi s·ª≠ d·ª•ng th∆∞ vi·ªán <code class="language-plaintext highlighter-rouge">sklearn</code>, kerrnel n√†y ƒë∆∞·ª£c ch·ªçn b·∫±ng c√°ch ƒë·∫∑t <code class="language-plaintext highlighter-rouge">kernel = 'poly'</code>. Th√¥ng tin c·ª• th·ªÉ v·ªÅ c√°ch s·ª≠ d·ª•ng c√≥ th·ªÉ xem <a href="http://scikit-learn.org/stable/modules/svm.html#svm-kernels">t·∫°i ƒë√¢y</a>.</p>

<p><a name="-radial-basic-function"></a></p>

<h4 id="323-radial-basic-function">3.2.3. Radial Basic Function</h4>
<p>Radial Basic Function (RBF) kernel hay Gaussian kernel ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh·∫•t trong th·ª±c t·∫ø, v√† l√† l·ª±a ch·ªçn m·∫∑c ƒë·ªãnh trong sklearn. N√≥ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·ªüi:
\[
k(\mathbf{x}, \mathbf{z}) = \exp(-\gamma ||\mathbf{x} - \mathbf{z}||_2^2), ~~ \gamma &gt; 0
\]</p>

<p>Trong sklearn, <code class="language-plaintext highlighter-rouge">kernel = 'rbf'</code>.</p>

<p><a name="-sigmoid"></a></p>

<h4 id="324-sigmoid">3.2.4. Sigmoid</h4>

<p><a href="/2017/01/27/logisticregression/#sigmoid-function">Sigmoid function</a> c≈©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m kernel:
\[
k(\mathbf{x}, \mathbf{z}) = \text{tanh}(\gamma \mathbf{x}^T\mathbf{z} + r)
\]</p>

<p><code class="language-plaintext highlighter-rouge">kernel = 'sigmoid'</code></p>

<p><a name="-bang-tom-tat-cac-kernel-thong-dung"></a></p>

<h4 id="325-b·∫£ng-t√≥m-t·∫Øt-c√°c-kernel-th√¥ng-d·ª•ng">3.2.5. B·∫£ng t√≥m t·∫Øt c√°c kernel th√¥ng d·ª•ng</h4>

<p>D∆∞·ªõi ƒë√¢y l√† b·∫£ng t√≥m t·∫Øt c√°c kernel th√¥ng d·ª•ng v√† c√°ch s·ª≠ d·ª•ng trong <code class="language-plaintext highlighter-rouge">sklearn</code>.</p>
<hr />

<table>
  <thead>
    <tr>
      <th><strong>T√™n</strong></th>
      <th><strong>C√¥ng th·ª©c</strong></th>
      <th><code class="language-plaintext highlighter-rouge">kernel</code></th>
      <th><strong>Thi·∫øt l·∫≠p h·ªá s·ªë</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>linear</td>
      <td>\(\mathbf{x}^T\mathbf{z}\)</td>
      <td><code class="language-plaintext highlighter-rouge">'linear'</code></td>
      <td>kh√¥ng c√≥ h·ªá s·ªë</td>
    </tr>
    <tr>
      <td>polynomial</td>
      <td>\((r + \gamma \mathbf{x}^T\mathbf{z})^d \)</td>
      <td><code class="language-plaintext highlighter-rouge">'poly'</code></td>
      <td>\(d\): <code class="language-plaintext highlighter-rouge">degree</code>, \(\gamma\): <code class="language-plaintext highlighter-rouge">gamma</code>, \(r\): <code class="language-plaintext highlighter-rouge">coef0</code></td>
    </tr>
    <tr>
      <td>sigmoid</td>
      <td>\(\text{tanh}(\gamma \mathbf{x}^T\mathbf{z} + r)\)</td>
      <td><code class="language-plaintext highlighter-rouge">'sigmoid'</code></td>
      <td>\(\gamma\): <code class="language-plaintext highlighter-rouge">gamma</code>, \(r\): <code class="language-plaintext highlighter-rouge">coef0</code></td>
    </tr>
    <tr>
      <td>rbf</td>
      <td>\(\exp(-\gamma ||\mathbf{x} - \mathbf{z}||_2^2)\)</td>
      <td><code class="language-plaintext highlighter-rouge">'rbf'</code></td>
      <td>\(\gamma &gt;0\): <code class="language-plaintext highlighter-rouge">gamma</code></td>
    </tr>
  </tbody>
</table>

<hr />

<p>N·∫øu b·∫°n mu·ªën s·ª≠ d·ª•ng c√°c th∆∞ vi·ªán cho C/C++, c√°c b·∫°n c√≥ th·ªÉ tham kh·∫£o <a href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM</a> v√† <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR</a></p>

<p><a name="-kernel-tu-dinh-nghia"></a></p>

<h4 id="326-kernel-t·ª±-ƒë·ªãnh-nghƒ©a">3.2.6. Kernel t·ª± ƒë·ªãnh nghƒ©a</h4>

<p>Ngo√†i c√°c h√†m kernel th√¥ng d·ª•ng nh∆∞ tr√™n, ch√∫ng ta c≈©ng c√≥ th·ªÉ t·ª± ƒë·ªãnh nghƒ©a c√°c kernel c·ªßa m√¨nh <a href="http://scikit-learn.org/stable/modules/svm.html#svm-kernels">nh∆∞ trong h∆∞·ªõng d·∫´n n√†y</a>. 
<a name="-vi-du-minh-hoa"></a></p>

<h2 id="4-v√≠-d·ª•-minh-h·ªça">4. V√≠ d·ª• minh h·ªça</h2>

<p><a name="-bai-toan-xor"></a></p>

<h3 id="41-b√†i-to√°n-xor">4.1. B√†i to√°n XOR</h3>
<p>Ch√∫ng ta c√πng quay l·∫°i v·ªõi b√†i to√°n XOR. Ch√∫ng ta bi·∫øt r·∫±ng <a href="/2017/02/24/mlp/#-pla-cho-cac-ham-logic-co-ban">b√†i to√°n XOR kh√¥ng th·ªÉ gi·∫£i quy·∫øt n·∫øu ch·ªâ d√πng m·ªôt b·ªô ph√¢n l·ªõp tuy·∫øn t√≠nh</a>. Neurrel Network c·∫ßn 2 layers ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n n√†y. V·ªõi SVM, ch√∫ng ta c√≥ c√°ch ƒë·ªÉ ch·ªâ c·∫ßn s·ª≠ d·ª•ng m·ªôt b·ªô ph√¢n l·ªõp. D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª•:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="c1"># XOR dataset and targets
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
          <span class="c1">#---
</span>          <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)].</span><span class="n">T</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
<span class="c1"># figure number
</span><span class="n">fignum</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># fit the model
</span><span class="k">for</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'sigmoid'</span><span class="p">,</span> <span class="s">'poly'</span><span class="p">,</span> <span class="s">'rbf'</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">coef0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">PdfPages</span><span class="p">(</span><span class="n">kernel</span> <span class="o">+</span> <span class="s">'2.pdf'</span><span class="p">)</span> <span class="k">as</span> <span class="n">pdf</span><span class="p">:</span>
        <span class="c1"># plot the line, the points, and the nearest vectors to the plane
</span>        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">fignum</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">clf</span><span class="p">()</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">clf</span><span class="p">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
                    <span class="n">facecolors</span><span class="o">=</span><span class="s">'None'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'ro'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'bs'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'tight'</span><span class="p">)</span>
        <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span>
        <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span>
        
        <span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mgrid</span><span class="p">[</span><span class="n">x_min</span><span class="p">:</span><span class="n">x_max</span><span class="p">:</span><span class="mf">200j</span><span class="p">,</span> <span class="n">y_min</span><span class="p">:</span><span class="n">y_max</span><span class="p">:</span><span class="mf">200j</span><span class="p">]</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="p">.</span><span class="n">ravel</span><span class="p">()])</span>

        <span class="c1"># Put the result into a color plot
</span>        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">fignum</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">CS</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">Z</span><span class="p">),</span> <span class="mi">200</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'jet'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="p">.</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s">'k'</span><span class="p">,</span> <span class="s">'k'</span><span class="p">,</span> <span class="s">'k'</span><span class="p">],</span> <span class="n">linestyles</span><span class="o">=</span><span class="p">[</span><span class="s">'--'</span><span class="p">,</span> <span class="s">'-'</span><span class="p">,</span> <span class="s">'--'</span><span class="p">],</span>
                    <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">)</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(())</span>
        <span class="n">fignum</span> <span class="o">=</span> <span class="n">fignum</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">pdf</span><span class="p">.</span><span class="n">savefig</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>K·∫øt qu·∫£ ƒë∆∞·ª£c cho trong H√¨nh 2 d∆∞·ªõi ƒë√¢y:</p>

<hr />

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/sigmoid2.png" />
         <br />
        a)
         </td>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/poly1.png" />
         <br />
        b)
        </td>

    </tr>
    <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/rbf1.png" />
         <br />
        c)
         </td>
        <td width="40%" style="border: 0px solid white" align="justify">
        H√¨nh 2: S·ª≠ d·ª•ng kerrnel SVM ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n XOR. a) sigmoid kernel. b) polynomial kernel. c) RBF kernel. C√°c ƒë∆∞·ªùng n√©t li·ªÅn l√† c√°c ƒë∆∞·ªùng ph√¢n l·ªõp, ·ª©ng v·ªõi gi√° tr·ªã c·ªßa bi·ªÉu th·ª©c \((6)\) b·∫±ng 0. C√°c ƒë∆∞·ªùng n√©t ƒë·ª©t l√† c√°c ƒë∆∞·ªùng ƒë·ªìng m·ª©c ·ª©ng v·ªõi gi√° tr·ªã c·ªßa bi·ªÉu th·ª©c \((6)\) b·∫±ng \(\pm 0.5\).
        Trong ba ph∆∞∆°ng ph√°p, RBF cho k·∫øt qu·∫£ t·ªët nh·∫•t v√¨ ch√∫ng cho k·∫øt qu·∫£ ƒë·ªëi x·ª©ng, h·ª£p l√Ω v·ªõi d·ªØ li·ªáu b√†i to√°n. 
        </td>
    </tr>
</table>
</div>
<hr />

<p>Ta c√≥ c√°c nh·∫≠n x√©t ƒë·ªëi v·ªõi m·ªói kernel nh∆∞ sau:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">sigmoid</code>: nghi·ªám t√¨m ƒë∆∞·ª£c kh√¥ng th·∫≠t t·ªët v√¨ c√≥ 3 trong 4 ƒëi·ªÉm n·∫±m ch√≠nh x√°c tr√™n ƒë∆∞·ªùng ph√¢n chia. N√≥i c√°ch kh√°c, nghi·ªám n√†y r·∫•t <em>nh·∫°y c·∫£m v·ªõi nhi·ªÖu</em>.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">poly</code>: Nghi·ªám n√†y c√≥ t·ªët h∆°n nghi·ªám c·ªßa <code class="language-plaintext highlighter-rouge">sigmoid</code> nh∆∞ng k·∫øt qu·∫£ c√≥ ph·∫ßn gi·ªëng v·ªõi <a href="/2017/03/04/overfitting/">overfitting</a>.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">rbf</code>: D·ªØ li·ªáu ƒë∆∞·ª£c t·∫°o ra m·ªôt c√°ch ƒë·ªëi x·ª©ng, ƒë∆∞·ªùng ph√¢n l·ªõp t√¨m ƒë∆∞·ª£c c≈©ng t·∫°o ra c√°c v√πng ƒë·ªëi x·ª©ng v·ªõi m·ªói class. Nghi·ªám n√†y ƒë∆∞·ª£c cho l√† <em>h·ª£p l√Ω h∆°n</em>. Tr√™n th·ª±c t·∫ø, c√°c <code class="language-plaintext highlighter-rouge">rbf</code> kernel ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh·∫•t v√† c≈©ng l√† l·ª±a ch·ªçn m·∫∑c ƒë·ªãnh trong h√†m <code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code>.</p>
  </li>
</ul>

<p><a name="-du-lieu-gan-linearly-separable"></a></p>

<h3 id="42-d·ªØ-li·ªáu-g·∫ßn-linearly-separable">4.2. D·ªØ li·ªáu g·∫ßn linearly separable</h3>

<p>X√©t m·ªôt v√≠ d·ª• kh√°c v·ªõi d·ªØ li·ªáu gi·ªØa hai classes l√† <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em> nh∆∞ H√ånh 3 d∆∞·ªõi ƒë√¢y:</p>
<hr />

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/sigmoid3.png" />
         <br />
        a)
         </td>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/poly3.png" />
         <br />
        b)
        </td>

    </tr>
    <tr>
        <td width="40%" style="border: 0px solid white" align="center">
        <img style="display:block;" width="100%" src="/assets/21_kernelsvm/rbf3.png" />
         <br />
        c)
         </td>
        <td width="40%" style="border: 0px solid white" align="justify">
        H√¨nh 3: S·ª≠ d·ª•ng kerrnel SVM ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n v·ªõi d·ªØ li·ªáu <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em>. a) sigmoid kernel. b) polynomial kernel. c) RBF kernel. C√°c ƒë∆∞·ªùng n√©t li·ªÅn l√† c√°c ƒë∆∞·ªùng ph√¢n l·ªõp, ·ª©ng v·ªõi gi√° tr·ªã c·ªßa bi·ªÉu th·ª©c \((6)\) b·∫±ng 0. C√°c ƒë∆∞·ªùng n√©t ƒë·ª©t l√† c√°c ƒë∆∞·ªùng ƒë·ªìng m·ª©c ·ª©ng v·ªõi gi√° tr·ªã c·ªßa bi·ªÉu th·ª©c \((6)\) b·∫±ng \(\pm 0.5\). V·ªõi b√†i to√°n n√†y, polynomial kernel cho k·∫øt qu·∫£ t·ªët h∆°n.
        
        </td>
    </tr>
</table>
</div>
<hr />

<p>Trong v√≠ d·ª• n√†y, <code class="language-plaintext highlighter-rouge">kernel = 'poly'</code> cho k·∫øt qu·∫£ t·ªët h∆°n <code class="language-plaintext highlighter-rouge">kernel = 'rbf'</code> v√¨ tr·ª±c quan cho ta th·∫•y r·∫±ng n·ª≠a b√™n ph·∫£i c·ªßa m·∫∑t ph·∫≥ng n√™n ho√†n tho√†n thu·ªôc v√†o class xanh. <code class="language-plaintext highlighter-rouge">sigmoid</code> kernel cho k·∫øt qu·∫£ kh√¥ng th·ª±c s·ª± t·ªët v√† √≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng.</p>

<p><a name="-bai-toan-phan-biet-gioi-tinh"></a></p>

<h3 id="43-b√†i-to√°n-ph√¢n-bi·ªát-gi·ªõi-t√≠nh">4.3. B√†i to√°n ph√¢n bi·ªát gi·ªõi t√≠nh</h3>
<p>B√†i to√°n n√†y ƒë√£ ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p ·ªü B√†i 12 v·ªõi d·ªØ li·ªáu ƒë·∫ßu v√†o l√† c√°c ·∫£nh khu√¥n m·∫∑t. V√¨ t√¥i kh√¥ng ƒë∆∞·ª£c ph√©p ph√¢n ph·ªëi c∆° s·ªü d·ªØ li·ªáu g·ªëc n√†y, t√¥i s·∫Ω chia s·∫ª cho c√°c b·∫°n v·ªÅ d·ªØ li·ªáu ƒë√£ qua x·ª≠ l√Ω, ƒë∆∞·ª£c l∆∞u trong file <code class="language-plaintext highlighter-rouge">myARgender.mat</code>, c√≥ th·ªÉ ƒë∆∞·ª£c <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/21_kernelsvm/plt/myARgender.mat">download t·∫°i ƒë√¢y</a>. D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng th∆∞ vi·ªán <code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code> ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.io</span> <span class="k">as</span> <span class="n">sio</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">sio</span><span class="p">.</span><span class="n">loadmat</span><span class="p">(</span><span class="s">'myARgender.mat'</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="s">'Y_train'</span><span class="p">].</span><span class="n">T</span> 
<span class="n">X_test</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="s">'Y_test'</span><span class="p">].</span><span class="n">T</span> 
<span class="n">N</span> <span class="o">=</span> <span class="mi">700</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="s">'label_train'</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="s">'label_test'</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'poly'</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: %.2f %%"</span> <span class="o">%</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 92.86 %
</code></pre></div></div>

<p>K·∫øt qu·∫£ kh√¥ng t·ªá! C√°c b·∫°n th·ª≠ thay c√°c <code class="language-plaintext highlighter-rouge">kernel</code> v√† thi·∫øt l·∫≠p c√°c tham s·ªë kh√°c xem k·∫øt qu·∫£ thay ƒë·ªïi nh∆∞ th·∫ø n√†o. V√¨ d·ªØ li·ªáu gi·ªØa hai classes l√† <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em> n√™n kh√¥ng c√≥ s·ª± kh√°c nhau nhi·ªÅu gi·ªØa c√°c kernel.</p>

<p><a name="-tom-tat"></a></p>

<h2 id="5-t√≥m-t·∫Øt">5. T√≥m t·∫Øt</h2>

<ul>
  <li>
    <p>N·∫øu d·ªØ li·ªáu c·ªßa hai l·ªõp l√† <em>kh√¥ng ph√¢n bi·ªát tuy·∫øn t√≠nh</em>, ch√∫ng ta c√≥ th·ªÉ t√¨m c√°ch bi·∫øn ƒë·ªïi d·ªØ li·ªáu sang m·ªôt kh√¥ng gian m·ªõi sao cho trong kh√¥ng gian m·ªõi ·∫•y, d·ªØ li·ªáu c·ªßa hai l·ªõp l√† <em>ph√¢n bi·ªát tuy·∫øn t√≠nh</em> ho·∫∑c <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em>.</p>
  </li>
  <li>
    <p>Vi·ªác t√≠nh to√°n tr·ª±c ti·∫øp h√†m \(\Phi()\) ƒë√¥i khi ph·ª©c t·∫°p v√† t·ªën nhi·ªÅu b·ªô nh·ªõ. Thay v√†o ƒë√≥, ta c√≥ th·ªÉ s·ª≠ d·ª•ng <strong>kernel trick</strong>. Trong c√°ch ti·∫øp c·∫≠n n√†y, ta ch·ªâ c·∫ßn t√≠nh t√≠ch v√¥ h∆∞·ªõng c·ªßa hai vector b·∫•t k·ª≥ trong kh√¥ng gian m·ªõi: \(k(\mathbf{x}, \mathbf{z}) = \Phi(\mathbf{x})^T\Phi(\mathbf{z})\).</p>
  </li>
  <li>
    <p>Th√¥ng th∆∞·ªùng, c√°c h√†m \(k()\) th·ªèa m√£n ƒëi·ªÅu ki·ªán Merrcer, v√† ƒë∆∞·ª£c g·ªçi l√† <em>kernel</em>. C√°ch gi·∫£i b√†i to√°n SVM v·ªõi kernel ho√†n to√†n gi·ªëng v·ªõi c√°ch gi·∫£i b√†i to√°n Soft Margin SVM.</p>
  </li>
  <li>
    <p>C√≥ 4 lo·∫°i kernel th√¥ng d·ª•ng: <code class="language-plaintext highlighter-rouge">linear</code>, <code class="language-plaintext highlighter-rouge">poly</code>, <code class="language-plaintext highlighter-rouge">rbf</code>, <code class="language-plaintext highlighter-rouge">sigmoid</code>. Trong ƒë√≥, <code class="language-plaintext highlighter-rouge">rbf</code> ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh·∫•t v√† l√† l·ª±a ch·ªçn m·∫∑c ƒë·ªãnh trong c√°c th∆∞ vi·ªán SVM.</p>
  </li>
  <li>
    <p>V·ªõi d·ªØ li·ªáu <em>g·∫ßn ph√¢n bi·ªát tuy·∫øn t√≠nh</em>, <code class="language-plaintext highlighter-rouge">linear</code> v√† <code class="language-plaintext highlighter-rouge">poly</code> kernels cho k·∫øt qu·∫£ t·ªët h∆°n.</p>
  </li>
  <li>
    <p><a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/21_kernelsvm/plt/KSVM.ipynb">Source code</a>.</p>
  </li>
</ul>

<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="6-t√†i-li·ªáu-tham-kh·∫£o">6. T√†i li·ªáu tham kh·∫£o</h2>
<p>[1] Bishop, Christopher M. ‚ÄúPattern recognition and Machine Learning.‚Äù, Springer  (2006). (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">book</a>)</p>

<p>[2] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley &amp; Sons, 2012.</p>

<p>[3] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"><code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code></a></p>

<p>[4] <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">LIBSVM ‚Äì A Library for Support Vector Machines</a></p>

<p>[5] Bennett, K. P. (1992). ‚Äú<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.3307&amp;rep=rep1&amp;type=pdf">Robust linear programming discrimination of two linearly separable sets</a>‚Äù. <em>Optimization Methods and Software</em> 1, 23‚Äì34.</p>

<p>[6] Sch¬®olkopf, B., A. Smola, R. C.Williamson, and P. L. Bartlett (2000). ‚Äú<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.2928&amp;rep=rep1&amp;type=pdf">New support vector algorithms</a>‚Äù. <em>Neural Computation 12</em>(5), 1207‚Äì1245</p>

<p>[7]  Rosasco, L.; De Vito, E. D.; Caponnetto, A.; Piana, M.; Verri, A. (2004). ‚Äú<a href="http://web.mit.edu/lrosasco/www/publications/loss.pdf">Are Loss Functions All the Same?</a>‚Äù. <em>Neural Computation</em>. 16 (5): 1063‚Äì1076</p>

<p>[8] <a href="http://scikit-learn.org/stable/modules/svm.html#svm-kernels">slearn Kernel functions</a></p>

<p>[9] <a href="https://en.wikipedia.org/wiki/Kernel_method">Kernel method</a></p>

<p>[10] <a href="http://www.support-vector-machines.org/">http://www.support-vector-machines.org/</a></p>
:ET