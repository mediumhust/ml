I"A<p>Các bài toán classification thực tế thường có rất nhiều classes (multi-class), các <a href="/2017/02/11/binaryclassifiers/#-binary-classifiers-cho-multi-class-classification-problems">binary classifiers mặc dù có thể áp dụng cho các bài toán multi-class</a>, chúng vẫn có những hạn chế nhất định. Với binary classifiers, kỹ thuật được sử dụng nhiều nhất <a href="/2017/02/11/binaryclassifiers/#one-vs-rest-hay-one-hot-coding"><strong>one-vs-rest</strong></a> có <a href="/2017/02/11/binaryclassifiers/#han-che-cua-one-vs-rest">một hạn chế về tổng các xác suất</a>. Trong post này, một phương pháp mở rộng của Logistic Regression sẽ được giới thiệu giúp khắc phục hạn chế trên. Một lần nữa, dù là Softmax <strong>Regression</strong>, phương pháp này được sử dụng rộng rãi như một phương pháp classification.</p>

<p><strong>Trong trang này:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Giới thiệu</a></li>
  <li><a href="#-softmax-function">2. Softmax function</a>
    <ul>
      <li><a href="#-cong-thuc-cua-softmax-function">2.1. Công thức của Softmax function</a></li>
      <li><a href="#-softmax-function-trong-python">2.2. Softmax function trong Python</a></li>
      <li><a href="#-mot-vai-vi-du">2.3. Một vài ví dụ</a></li>
      <li><a href="#-phien-ban-on-dinh-hon-cua-softmax-function">2.4. Phiên bản ổn định hơn của softmax function</a></li>
    </ul>
  </li>
  <li><a href="#-ham-mat-mat-va-phuong-phap-toi-uu">3. Hàm mất mát và phương pháp tối ưu</a>
    <ul>
      <li><a href="#-one-hot-coding">3.1. One hot coding</a></li>
      <li><a href="#-cross-entropy">3.2. Cross Entropy</a></li>
      <li><a href="#-ham-mat-mat-cho-softmax-regression">3.3. Hàm mất mát cho Softmax Regression</a></li>
      <li><a href="#-toi-uu-ham-mat-mat">3.4. Tối ưu hàm mất mát</a></li>
      <li><a href="#-logistic-regression-la-mot-truong-hop-dat-biet-cua-softmax-regression">3.5. Logistic Regression là một trường hợp đặt biệt của Softmax Regression</a></li>
    </ul>
  </li>
  <li><a href="#-mot-vai-luu-y-khi-lap-trinh-voi-python">4. Một vài lưu ý khi lập trình với Python</a>
    <ul>
      <li><a href="#-bat-dau-voi-du-lieu-nho">4.1. Bắt đầu với dữ liệu nhỏ</a></li>
      <li><a href="#-ma-tran-one-hot-coding">4.2. Ma trận one-hot coding</a></li>
      <li><a href="#-kiem-tra-dao-ham">4.3. Kiểm tra đạo hàm</a></li>
      <li><a href="#-ham-chinh-cho-training-softmax-regression">4.4. Hàm chính cho training Softmax Regression</a></li>
      <li><a href="#-ham-du-doan-class-cho-du-lieu-moi">4.5. Hàm dự đoán class cho dữ liệu mới</a></li>
    </ul>
  </li>
  <li><a href="#-vi-du-voi-python">5. Ví dụ với Python</a>
    <ul>
      <li><a href="#-simulated-data">5.1. Simulated data</a></li>
      <li><a href="#-softmax-regression-cho-mnist">5.2. Softmax Regression cho MNIST</a></li>
    </ul>
  </li>
  <li><a href="#-thao-luan">6. Thảo luận</a>
    <ul>
      <li><a href="#-boundary-tao-boi-softmax-regression-la-linear">6.1 Boundary tạo bởi Softmax Regression là linear</a></li>
      <li><a href="#-softmax-regression-la-mot-trong-hai-classifiers-pho-bien-nhat">6.2. Softmax Regression là một trong hai classifiers phổ biến nhất</a></li>
      <li><a href="#-source-code">6.3. Source code</a></li>
    </ul>
  </li>
  <li><a href="#tai-lieu-tham-khao">Tài liệu tham khảo</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><strong>Một lưu ý nhỏ:</strong> Hàm mất mát của Softmax Regression trông có vẻ khá phức tạp, nhưng nếu  kiên trì đọc đến phần phương pháp tối ưu, các bạn sẽ thấy vẻ đẹp ẩn sau sự phức tạp đó. Gradient của hàm mất mát và công thức cập nhật ma trận trọng số là rất đơn giản. (Đơn giản sau vài bước biến đổi toán học <em>trông có vẻ</em> phức tạp).</p>

<p>Nếu có điểm nào khó hiểu, bạn đọc được khuyến khích đọc lại các bài trước, trong đó quan trọng nhất là <a href="/2017/01/27/logisticregression/">Bài 10: Logistic Regression</a>.
<a name="-gioi-thieu"></a></p>

<h2 id="1-giới-thiệu">1. Giới thiệu</h2>
<p>Tôi xin phép được bắt đầu từ mô hình <a href="/2017/02/11/binaryclassifiers/#one-vs-rest-hay-one-hot-coding"><strong>one-vs-rest</strong></a> được trình bày trong bài trước. Output layer (màu đỏ nhạt) có thể phân tách thành hai <em>sublayer</em> như hình dưới đây:</p>

<div class="imgcap">
<img src="\assets\13_softmax\onevsrest.png" align="center" width="600" />
<div class="thecap">Hình 1: Multi-class classification với Logistic Regression và one-vs-rest.</div>
</div>

<p>Dữ liệu \(\mathbf{x}\) có số chiều là \((d +1)\) vì có phần tử 1 được thêm vào phía trước, thể hiện hệ số tự do trong hàm tuyến tính. Hệ số tự do \(w_{0j}\) còn được gọi là bias.</p>

<p>Giả sử số classes là \(C\). Với one-vs-rest, chúng ta cần xây dựng \(C\) Logistic Regression khác nhau. Các <em>đầu ra dự đoán</em> được tính theo hàm sigmoid:
\[
a_i = \text{sigmoid}(z_i) = \text{sigmoid}(\mathbf{w}_i^T\mathbf{x})
\]
Trong kỹ thuật này, các phần tử \(a_i, i = 1, 2, \dots, C\) được suy ra trực tiếp chỉ với \(z_i\). Vì vậy, không có mối quan hệ chặt chẽ nào giữa các \(a_i\), tức tổng của chúng có thể nhỏ hơn hoặc lớn hơn 1. Nếu ta có thể khai thác được mỗi quan hệ giữa các \(z_i\) thì kết quả của bài toán classification có thể tốt hơn.</p>

<p>Chú ý rằng các mô hình Linear Regression, PLA, Logistic Regression chỉ có 1 node ở output layer. Trong các trường hợp đó, tham số mô hình chỉ là 1 vector \(\mathbf{w}\). Trong trường hợp output layer có nhiều hơn 1 node, tham số mô hình sẽ là tập hợp 
tham số \(\mathbf{w}_i\) ứng với từng node. Lúc này, ta có <em>ma trận trọng số</em> \(\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_C]\).</p>

<p><a name="-softmax-function"></a></p>

<h2 id="2-softmax-function">2. Softmax function</h2>
<p><a name="-cong-thuc-cua-softmax-function"></a></p>

<h3 id="21-công-thức-của-softmax-function">2.1. Công thức của Softmax function</h3>
<p>Chúng ta cần một mô hình xác suất sao cho với mỗi input \(\mathbf{x}\), \(a_i\) thể hiện xác suất để input đó rơi vào class \(i\). Vậy điều kiện cần là các \(a_i\) phải dương và tổng của chúng bằng 1. Để có thể thỏa mãn điều kiện này, chúng ta cần <em>nhìn vào</em> mọi giá trị \(z_i\) và dựa trên quan hệ giữa các \(z_i\) này để tính toán giá trị của \(a_i\). Ngoài các điều kiện \(a_i\) lớn hơn 0 và có tổng bằng 1, chúng ta sẽ thêm một điều kiện cũng rất tự nhiên nữa, đó là: giá trị \(z_i = \mathbf{w}_i^T\mathbf{x}\) càng lớn thì xác suất dữ liệu rơi vào class \(i\) càng cao. Điều kiện cuối này chỉ ra rằng chúng ta cần một hàm đồng biến ở đây.</p>

<p>Chú ý rằng \(z_i \) có thể nhận giá trị cả âm và dương. Một hàm số <em>mượt</em> đơn giản có thể chắc chắn biến  \(z_i \) thành một giá trị dương, và hơn nữa, đồng biến, là hàm \(\exp(z_i) = e^{z_i}\). Điều kiện <em>mượt</em> để thuận lợi hơn trong việc tính đạo hàm sau này. Điều kiện cuối cùng, tổng các \(a_i\) bằng 1 có thể được đảm bảo nếu:</p>

<p>\[
a_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}, ~~ \forall i = 1, 2, \dots, C
\]</p>

<p>Hàm số này, tính tất cả các \(a_i\) dựa vào tất cả các \(z_i\), thõa mãn tất cả các điều kiện đã xét: dương, tổng bằng 1, giữ được <em>thứ tự</em> của \(z_i\). Hàm số này được gọi là <em>softmax function</em>. Chú ý rằng với cách định nghĩa này, không có xác suất \(a_i\) nào tuyệt đối bằng 0 hoặc tuyệt đối bằng 1, mặc dù chúng có thể rất gần 0 hoặc 1 khi \(z_i\) rất nhỏ hoặc rất lớn khi so sánh với các \(z_j, j \neq i\).</p>

<p>Lúc này, ta có thể giả sử rằng:</p>

<p>\[
P(y_k = i | \mathbf{x}_k; \mathbf{W}) = a_i
\]</p>

<p>Trong đó, \(P(y = i | \mathbf{x}; \mathbf{W})\) được hiểu là xác suất để một điểm dữ liệu \(\mathbf{x}\) rơi vào class thứ \(i\) nếu biết tham số mô hình (ma trận trọng số) là \(\mathbf{W}\).</p>

<p>Hình vẽ dưới đây thể hiện mạng Softmax Regression dưới dạng neural network:</p>
<div class="imgcap">
<img src="\assets\13_softmax\softmax_nn.png" align="center" width="800" />
<div class="thecap">Hình 2: Mô hình Softmax Regression dưới dạng Neural network.</div>
</div>

<p>Ở phần bên phải, hàm tuyến tính \(\Sigma\) và hàm softmax (activation function) được tách riêng ra để phục vụ cho mục đích minh họa. Dạng <em>short form</em> ở bên phải là dạng hay được sử dụng trong các Neural Networks, lớp \(\mathbf{a}\) được ngầm hiểu là bao gồm cả lớp \(\mathbf{z}\).</p>

<p><a name="-softmax-function-trong-python"></a></p>

<h3 id="22-softmax-function-trong-python">2.2. Softmax function trong Python</h3>
<p>Dưới đây là một đoạn code viết hàm softmax. Đầu vào là một ma trận với mỗi cột là một vector \(\mathbf{z}\), đầu ra cũng là một ma trận mà mỗi cột có giá trị là \(\mathbf{a} = \text{softmax}(\mathbf{z})\). Các giá trị của \(\mathbf{z}\) còn được gọi là <strong>scores</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="s">"""
    Compute softmax values for each sets of scores in V.
    each column of V is a set of score.    
    """</span>
    <span class="n">e_Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">e_Z</span> <span class="o">/</span> <span class="n">e_Z</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">A</span>
</code></pre></div></div>

<p><a name="-mot-vai-vi-du"></a></p>

<h3 id="23-một-vài-ví-dụ">2.3. Một vài ví dụ</h3>

<p>Hình 3 dưới đây là một vài ví dụ về mối quan hệ giữa đầu vào và đầu ra của hàm softmax. Hàng trên màu xanh nhạt thể hiện các scores \(z_i\) với giả sử rằng số classes là 3. Hàng dưới màu đỏ nhạt thể hiện các giá trị đầu ra \(a_i\) của hàm softmax.</p>

<div class="imgcap">
<img src="\assets\13_softmax\softmax_ex.png" align="center" width="600" />
<div class="thecap">Hình 3: Một số ví dụ về đầu vào và đầu ra của hàm softmax.</div>
</div>

<p>Có một vài quan sát như sau:</p>

<ul>
  <li>
    <p>Cột 1: Nếu các \(z_i\) bằng nhau, thì các \(a_i\) cũng bằng nhau và bằng 1/3.</p>
  </li>
  <li>
    <p>Cột 2: Nếu giá trị lớn nhất trong các \(z_i\) là \(z_1\) vẫn bằng 2, nhưng các giá trị khác thay đổi, thì mặc dù xác suất tương ứng \(a_1\) vẫn là lớn nhất, nhưng nó đã thay đổi lên hơn 0.5. Đây chính là một lý do mà tên của hàm này có từ <em>soft</em>. (<em>max</em> vì phẩn từ lớn nhất vẫn là phần tử lớn nhất).</p>
  </li>
  <li>
    <p>Cột 3: Khi các giá trị \(z_i\) là âm thì các giá trị \(a_i\) vẫn là dương và thứ tự vẫn được đảm bảo.</p>
  </li>
  <li>
    <p>Cột 4: Nếu \(z_1 = z_2\), thì \(a_1 = a_2\).</p>
  </li>
</ul>

<p>Bạn đọc có thể thử với các giá trị khác trực tiếp trên trình duyệt trong <a href="http://neuralnetworksanddeeplearning.com/chap3.html">link này</a>, kéo xuống phần Softmax.</p>

<p><a name="-phien-ban-on-dinh-hon-cua-softmax-function"></a></p>

<h3 id="24-phiên-bản-ổn-định-hơn-của-softmax-function">2.4. Phiên bản ổn định hơn của softmax function</h3>

<p>Khi một trong các \(z_i\) quá lớn, việc tính toán \(\exp(z_i)\) có thể gây ra hiện tượng tràn số (overflow), ảnh hưởng lớn tới kết quả của hàm softmax. Có một cách khắc phục hiện tượng này bằng cách dựa trên quan sát sau:</p>

<p>\[
\begin{eqnarray}
\frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)} &amp;=&amp; \frac{\exp(-c)\exp(z_i)}{\exp(-c)\sum_{j=1}^C \exp(z_j)}\newline
&amp;=&amp; \frac{\exp(z_i-c)}{\sum_{j=1}^C \exp(z_j-c)}
\end{eqnarray}
\]
với \(c\) là một hằng số bất kỳ.</p>

<p>Vậy một phương pháp đơn giản giúp khắc phục hiện tượng overflow là trừ tất cả các \(z_i\) đi một giá trị đủ lớn. Trong thực nghiệm, giá trị đủ lớn này thường được chọn là \(c = \max_i z_i\). Vậy chúng ta có thể sửa đoạn code cho hàm <code class="language-plaintext highlighter-rouge">softmax</code> phía trên bằng cách trừ mỗi cột của ma trận đầu vào <code class="language-plaintext highlighter-rouge">Z</code> đi giá trị lớn nhất trong cột đó. Ta có phiên bản ổn định hơn là <code class="language-plaintext highlighter-rouge">softmax_stable</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_stable</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="s">"""
    Compute softmax values for each sets of scores in Z.
    each column of Z is a set of score.    
    """</span>
    <span class="n">e_Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">))</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">e_Z</span> <span class="o">/</span> <span class="n">e_Z</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">A</span>
</code></pre></div></div>

<p>trong đó <code class="language-plaintext highlighter-rouge">axis = 0</code> nghĩa là lấy <code class="language-plaintext highlighter-rouge">max</code> theo cột (<code class="language-plaintext highlighter-rouge">axis = 1</code> sẽ lấy max theo hàng), <code class="language-plaintext highlighter-rouge">keepdims = True</code> để đảm bảo phép trừ giữa ma trận <code class="language-plaintext highlighter-rouge">Z</code> và vector  thực hiện được.</p>

<p><a name="-ham-mat-mat-va-phuong-phap-toi-uu"></a></p>

<h2 id="3-hàm-mất-mát-và-phương-pháp-tối-ưu">3. Hàm mất mát và phương pháp tối ưu</h2>

<p><a name="-one-hot-coding"></a></p>

<h3 id="31-one-hot-coding">3.1. One hot coding</h3>
<p>Với cách biểu diễn network như trên, mỗi output sẽ không còn là một giá trị tương ứng với mỗi class nữa mà sẽ là một vector có đúng 1 phần tử bằng 1, các phần tử còn lại bằng 0. Phần tử bằng 1 năm ở vị trí tương ứng với class đó, thể hiện rằng điểm dữ liệu đang xét rơi vào class này với xác suất bằng 1 (<em>sự thật</em> là như thế, không cần dự đoán). Cách <em>mã hóa</em> output này chính là <em>one-hot coding</em> mà tôi đã đề cập trong bài <a href="/2017/01/01/kmeans/">K-means clustering</a> và <a href="/2017/02/11/binaryclassifiers/#one-vs-rest-hay-one-hot-coding">bài trước</a>.</p>

<p>Khi sử dụng mô hình Softmax Regression, với mỗi đầu vào \(\mathbf{x}\), ta sẽ có <em>đầu ra dự đoán</em> là \(\mathbf{a} = \text{softmax}(\mathbf{W}^T\mathbf{x})\). Trong khi đó, <em>đầu ra thực sự</em> chúng ta có là vector \(\mathbf{y}\) được biểu diễn dưới dạng one-hot coding.</p>

<p>Hàm mất mát sẽ được xây dựng để tối thiểu sự khác nhau giữa <em>đầu ra dự đoán</em> \(\mathbf{a}\) và <em>đầu ra thực sự</em> \(\mathbf{y}\). Một lựa chọn đầu tiên ta có thể nghĩ tới là:</p>

<p>\[
J(\mathbf{W}) = \sum_{i=1}^N ||\mathbf{a}_i - \mathbf{y}_i||_2^2
\]
<strong>Tuy nhiên đây chưa phải là một lựa chọn tốt</strong>. Khi đánh giá sự khác nhau (hay khoảng cách) giữa hai phân bố xác suất (probability distributions), chúng ta có một đại lượng đo đếm khác hiệu quả hơn. Đại lượng đó có tên là <a href="https://en.wikipedia.org/wiki/Cross_entropy"><strong>cross entropy</strong></a>.</p>

<p><a name="-cross-entropy"></a></p>

<h3 id="32-cross-entropy">3.2. Cross Entropy</h3>
<p>Cross entropy giữa hai phân phối \(\mathbf{p}\) và \(\mathbf{q}\) được định nghĩa là:
\[
H(\mathbf{p}, \mathbf{q}) = \mathbf{E_p}[-\log \mathbf{q}]
\]</p>

<p>Với \(\mathbf{p}\) và \(\mathbf{q}\) là rời rạc (như \(\mathbf{y}\) và \(\mathbf{a}\) trong bài toán của chúng ta), công thức này được viết dưới dạng:</p>

<p>\[
H(\mathbf{p}, \mathbf{q}) =-\sum_{i=1}^C p_i \log q_i ~~~ (1)
\]</p>

<p>Để hiểu rõ hơn ưu điểm của hàm cross entropy và hàm bình phương khoảng cách thông thường, chúng ta cùng xem Hình 4 dưới đây. Đây là ví dụ trong trường hợp \(C = 2\) và \(p_1\) lần lượt nhận các giá trị \(0.5, 0.1\) và \(0.8\).</p>

<div>
<table width="100%" style="border: 0px solid white">
   <tr>
        <td width="30%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="/assets/13_softmax/crossentropy1.png" />
         </td>
        <td width="30%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/13_softmax/crossentropy2.png" />
        </td>
        <td width="30%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="/assets/13_softmax/crossentropy3.png" />
        </td>
    </tr>
</table> 
<div class="thecap"> Hình 4: So sánh giữa hàm cross entropy và hàm bình phương khoảng cách. Các điểm màu xanh lục thể hiện các giá trị nhỏ nhất của mỗi hàm. </div>
</div>

<p>Có hai nhận xét quan trọng sau đây:</p>

<ul>
  <li>
    <p>Giá trị nhỏ nhất của cả hai hàm số đạt được khi \(q = p\) tại hoành độ của các điểm màu xanh lục.</p>
  </li>
  <li>
    <p>Quan trọng hơn, hàm cross entropy nhận giá trị rất cao (tức loss rất cao) khi \(q\) ở xa \(p\). Trong khi đó, sự chênh lệch giữa các loss ở gần hay xa nghiệm của hàm bình phương khoảng cách \((q - p)^2\) là không đáng kể. Về mặt tối ưu, hàm cross entropy sẽ cho nghiệm <em>gần</em> với \(p\) hơn vì những nghiệm ở xa bị <em>trừng phạt</em> rất nặng.</p>
  </li>
</ul>

<p>Hai tính chất trên đây khiến cho cross entropy được sử dụng rộng rãi khi tính khoảng cách giữa hai phân phối xác suất.</p>

<p><strong>Chú ý:</strong> Hàm cross entropy không có tính đối xứng \(H(\mathbf{p}, \mathbf{q}) \neq H(\mathbf{q}, \mathbf{p})\). Điều này có thể dễ dàng nhận ra ở việc các thành phần của \(\mathbf{p}\) trong công thức \((1)\) có thể nhận giá trị bằng 0, trong khi đó các thành phần của \(\mathbf{q}\) phải là dương vì \(\log(0)\) không xác định. Chính vì vậy, khi sử dụng cross entropy trong các bài toán supervised learning, \(\mathbf{p}\) thường là <em>đầu ra thực sự</em> vì đầu ra thực sự chỉ có 1 thành phần bằng 1, còn lại bằng 0 (one-hot), \(\mathbf{q}\) thường là <em>đầu ra dự đoán</em>, khi mà không có xác suất nào tuyệt đối bằng 1 hoặc tuyệt đối bằng 0 cả.</p>

<p>Trong <a href="/2017/01/27/logisticregression/">Logistic Regression</a>, chúng ta cũng có hai phân phối đơn giản. (i) <em>Đầu ra thực sự</em> của điểm dữ liệu đầu vào \(\mathbf{x}_i\) có phân phối xác suất là \([y_i; 1 - y_i]\) với \(y_i\) là xác suất để điểm dữ liệu đầu vào rơi vào class thứ nhất (bằng 1 nếu \(y_i = 1\), bằng 0 nếu \(y_i = 0\)). (ii). <em>Đầu ra dự đoán</em> của điểm dữ liệu đó là \(a_i = \text{sigmoid}(\mathbf{w}^T\mathbf{x})\) là xác suất để điểm đó rơi vào class thứ nhất. Xác suất để điểm đó rơi vào class thứ hai có thể được dễ dàng suy ra lf \(1 - a_i\). Vì vậy, hàm mất mát trong Logistic Regression:
\[
J(\mathbf{w}) = -\sum_{i=1}^N(y_i \log {a}_i + (1-y_i) \log (1 - {a}_i))
\]
chính là một trường hợp đặc biệt của Cross Entropy. (\(N\) được dùng để thể hiện số điểm dữ liệu trong tập training).</p>

<p>Với Softmax Regression, trong trường hợp có \(C\) classes, <em>loss</em> giữa đầu ra dự đoán và đầu ra thực sự của một điểm dữ liệu \(\mathbf{x}_i\) được tính bằng:
\[
J(\mathbf{W};\mathbf{x}_i, \mathbf{y}_i) = -\sum_{j=1}^C y_{ji}\log(a_{ji})
\]
Với \(y_{ji}\) và \( a_{ji}\) lần lượt là là phần tử thứ \(j\) của vector (xác suất) \(\mathbf{y}_i\) và \(\mathbf{a}_i\). Nhắc lại rằng đầu ra \(\mathbf{a}_i\) phụ thuộc vào đầu vào \(\mathbf{x}_i\) và ma trận trọng số \(\mathbf{W}\).</p>

<p><a name="-ham-mat-mat-cho-softmax-regression"></a></p>

<h3 id="33-hàm-mất-mát-cho-softmax-regression">3.3. Hàm mất mát cho Softmax Regression</h3>
<p>Kết hợp tất cả các cặp dữ liệu \(\mathbf{x}_i, \mathbf{y}_i, i = 1, 2, \dots, N\), chúng ta sẽ có hàm mất mát cho Softmax Regression như sau:</p>

<p>\[
\begin{eqnarray}
J(\mathbf{W}; \mathbf{X}, \mathbf{Y}) = -\sum_{i = 1}^N \sum_{j = 1}^C y_{ji}\log(a_{ji}) \newline
= -\sum_{i = 1}^N \sum_{j = 1}^C y_{ji}\log\left(\frac{\exp(\mathbf{w}_j^T\mathbf{x}_i)}{\sum_{k=1}^C \exp(\mathbf{w}_k^T\mathbf{x}_i)}\right)
\end{eqnarray}
\]</p>

<p>Với ma trận trọng số \(\mathbf{W}\) là biến cần tối ưu. Hàm mất mát này trông <em>có vẻ đáng sợ</em>, nhưng đừng sợ, đọc tiếp các bạn sẽ thấy đạo hàm của nó rất đẹp (<em>và đáng yêu</em>).</p>

<p><a name="-toi-uu-ham-mat-mat"></a></p>

<h3 id="34-tối-ưu-hàm-mất-mát">3.4. Tối ưu hàm mất mát</h3>

<p>Một lần nữa, chúng ta lại sử dụng <a href="/2017/01/16/gradientdescent2/#-stochastic-gradient-descent">Stochastic Gradient Descent (SGD)</a> ở đây.</p>

<p>Với chỉ một cặp dữ liệu \((\mathbf{x}_i, \mathbf{y}_i)\), ta có: 
\[
J_i(\mathbf{W}) \triangleq J(\mathbf{W}; \mathbf{x}_i, \mathbf{y}_i) = 
\]
\[
\begin{eqnarray}
&amp;=&amp; -\sum_{j = 1}^C y_{ji}\log\left(\frac{\exp(\mathbf{w}_j^T\mathbf{x}_i)}{\sum_{k=1}^C \exp(\mathbf{w}_k^T\mathbf{x}_i)}\right) \newline
&amp;=&amp; -\sum_{j=1}^C\left(y_{ji} \mathbf{w}_j^T\mathbf{x}_i - y_{ji}\log\left(\sum_{k=1}^C \exp(\mathbf{w}_k^T\mathbf{x}_i)\right)\right) \newline
&amp;=&amp; -\sum_{j=1}^C y_{ji} \mathbf{w}_j^T\mathbf{x}_i + \log\left(\sum_{k=1}^C \exp(\mathbf{w}_k^T\mathbf{x}_i)\right) ~~ (3)
\end{eqnarray}
\]</p>

<p>trong biến đổi ở dòng cuối cùng, tôi đã sử dụng quan sát: \(\sum_{j=1}^C y_{ji} = 1\) vì nó là tổng các xác suất.</p>

<p>Tiếp theo ta sử dụng công thức: 
\[
\frac{\partial J_i(\mathbf{W})}{\partial \mathbf{W}} = \left[\frac{\partial J_i(\mathbf{W})}{\partial \mathbf{w}_1}, \frac{\partial J_i(\mathbf{W})}{\partial \mathbf{w}_2}, \dots, \frac{\partial J_i(\mathbf{W})}{\partial \mathbf{w}_C}    \right]~~(4)
\]</p>

<p>Trong đó, gradient theo từng cột có thể tính được dựa theo \((3)\):</p>

<p>\[
\begin{eqnarray}
\frac{\partial J_i(\mathbf{W})}{\partial \mathbf{w}_j} &amp;=&amp; -y_{ji}\mathbf{x}_i + 
\frac{\exp(\mathbf{w}_j^T\mathbf{x}_i)}{\sum_{k = 1}^C \exp(\mathbf{w}_k^T\mathbf{x}_i)}\mathbf{x}_i \newline
&amp;=&amp; -y_{ji}\mathbf{x}_i + a_{ji} \mathbf{x}_i = \mathbf{x}_i (a_{ji} - y_{ji}) \newline
&amp;=&amp; e_{ji}\mathbf{x}_{i} ~(\text{where}~ e_{ji} = a_{ji} - y_{ji}) ~~(5)
\end{eqnarray}
\]</p>

<p>Giá trị \(e_{ji} = a_{ji} - y_{ji} \) có thể được coi là <em>sai số dự đoán</em>.</p>

<p>Đến đây ta đã được biểu thức rất đẹp rồi. Kết hợp \((4)\) và \((5)\) ta có: 
\[
\frac{\partial J_i(\mathbf{W})}{\partial \mathbf{W}} = \mathbf{x}_i [e_{1i}, e_{2i}, \dots, e_{Ci}] = \mathbf{x}_i\mathbf{e}_i^T
\]
<a name="vi-du-va-luu-y-khi-lap-trinh-voi-python"></a></p>

<p>Từ đây ta cũng có thể suy ra rằng:
\[
\frac{\partial J(\mathbf{W})}{\partial \mathbf{W}} = \sum_{i=1}^N \mathbf{x}_i\mathbf{e}_i^T = \mathbf{X}\mathbf{E}^T
\]
với \(\mathbf{E} = \mathbf{A - Y}\). Công thức tính gradient đơn giản thế này giúp cho cả <a href="/2017/01/16/gradientdescent2/#-bien-the-cua-gradient-descent">Batch Gradient Descent, Stochastic Gradient Descent (SGD), và Mini-batch Gradient Descent</a> đều có thể dễ dàng được áp dụng.</p>

<p>Giả sử rằng chúng ta sử dụng SGD, công thức cập nhật cho ma trận trọng số \(\mathbf{W}\) sẽ là: 
\[
\mathbf{W} = \mathbf{W} +\eta \mathbf{x}_{i}(\mathbf{y}_i - \mathbf{a}_i)^T
\]</p>

<p>Bạn có thấy công thức này giống với <a href="/2017/01/27/logisticregression/#cong-thuc-cap-nhat-cho-logistic-sigmoid-regression">công thức cập nhật của Logistic Regression</a> không!</p>

<p>Thực ra:</p>

<p><a name="-logistic-regression-la-mot-truong-hop-dat-biet-cua-softmax-regression"></a></p>

<h3 id="35-logistic-regression-là-một-trường-hợp-đặt-biệt-của-softmax-regression">3.5. Logistic Regression là một trường hợp đặt biệt của Softmax Regression</h3>

<p>Khi \(C = 2\), Softmax Regression và Logistic Regression là giống nhau. Thật vậy, đầu ra dự đoán của Softmax Regression với \(C= 2\) có thể được viết dưới dạng: 
\[
\begin{eqnarray}
a_1 &amp;=&amp; \frac{\exp(\mathbf{w}_1^T\mathbf{x})} {\exp(\mathbf{w}_1^T\mathbf{x}) + \exp(\mathbf{w}_2^T\mathbf{x})} \newline
&amp;=&amp; \frac{1}{1 + \exp((\mathbf{w}_2 - \mathbf{w}_1)^T\mathbf{x})}
\end{eqnarray}
\]</p>

<p>Đây chính là <a href="/2017/01/27/logisticregression/#sigmoid-function">sigmoid function</a>, là đầu ra dự đoán theo Logistic Regression. Khi \(C = 2\), bạn đọc cũng có thể thấy rằng hàm mất mát của Logistic và Softmax Regression đều là cross entropy. Hơn nữa, mặc dù có 2 outputs, Softmax Regression có thể rút gọn thành 1 output vì tổng 2 outputs luôn luôn bằng 1.</p>

<p>Softmax Regression còn có các tên gọi khác là Multinomial Logistic Regression, Maximum Entropy Classifier, hay rất nhiều tên khác nữa. Xem thêm <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">Multinomial logistic regression - Wikipedia</a>
<a name="-mot-vai-luu-y-khi-lap-trinh-voi-python"></a></p>

<h2 id="4-một-vài-lưu-ý-khi-lập-trình-với-python">4. Một vài lưu ý khi lập trình với Python</h2>

<p><a name="-bat-dau-voi-du-lieu-nho"></a></p>

<h3 id="41-bắt-đầu-với-dữ-liệu-nhỏ">4.1. Bắt đầu với dữ liệu nhỏ</h3>
<p>Các bài toán Machine Learning thường có độ phức tạp cao với lượng dữ liệu lớn và nhiều chiều. Để có thể áp dụng một thuật toán vào một bài toán cụ thể, trước tiên chúng ta cần áp dụng thuật toán đó vào <em>simulated data</em> (dữ liệu giả) với số chiều và số điểm dữ liệu nhỏ hơn. <em>Simulated data</em> này thường được tạo ngẫu nhiên (có thể thêm vài ràng buộc tùy vào đặc thù của dữ liệu). Với <em>simulated data</em> nhỏ, chúng ta có thể debug nhanh hơn và thử với nhiều trường hợp <em>simulated data</em> khác nhau. Khi nào thấy thuật toán chạy đúng chúng ta mới đưa <em>dữ liệu thật</em> vào.</p>

<p>Với Softmax Regression, tôi tạo <em>simulated data</em> như sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="c1"># randomly generate data 
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of training sample 
</span><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># data dimension 
</span><span class="n">C</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># number of classes 
</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,))</span>
</code></pre></div></div>
<p>Trong ví dụ đơn giản này, số điểm dữ liệu chỉ là <code class="language-plaintext highlighter-rouge">N = 2</code>, số chiều dữ liệu <code class="language-plaintext highlighter-rouge">d = 2</code>, và số classes <code class="language-plaintext highlighter-rouge">C = 3</code>. Những giá trị đủ nhỏ này giúp cho việc kiểm tra có thể được thực hiện một cách tức thì. Sau khi thuật toán chạy đúng với những giá trị nhỏ này, ta có thể thay <code class="language-plaintext highlighter-rouge">N, d, C</code> bằng vài giá trị khác trước khi sử dụng dữ liệu thật.</p>

<p><a name="-ma-tran-one-hot-coding"></a></p>

<h3 id="42-ma-trận-one-hot-coding">4.2. Ma trận one-hot coding</h3>
<p>Có một bước quan trọng nữa trong Softmax Regression là phải chuyển đổi mỗi label \(y_i\) thành một vector \(\mathbf{y}_i\) dưới dạng one-hot coding. Trong đó, chỉ có đúng một phần tử của \(\mathbf{y}_i\) bằng 1, các phần tử còn lại bằng 0. Như vậy, với \(N\) điểm dữ liệu và \(C\) classes, chúng ta sẽ có một ma trận có kích thước \(C \times N\) trong đó mỗi cột chỉ có đúng 1 phần tử bằng 1, còn lại bằng 0. Nếu chúng ta lưu toàn bộ dữ liệu này thì sẽ bị lãng phí bộ nhớ.</p>

<p>Một cách thường được sử dụng là lưu ma trận output \(\mathbf{Y}\) dưới dạng <em>sparse matrix</em>. Về cơ bản, cách làm này chỉ lưu các <strong>vị trí</strong> khác 0 của ma trận và <strong>giá trị</strong> khác 0 đó.</p>

<p>Python có hàm <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html">scipy.sparse.coo_matrix</a> giúp chúng ta thực hiện việc này. Với one-hot coding, tôi thực hiện như sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## One-hot coding
</span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span> 
<span class="k">def</span> <span class="nf">convert_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="p">):</span>
    <span class="s">"""
    convert 1d label to a matrix label: each column of this 
    matrix coresponding to 1 element in y. In i-th column of Y, 
    only one non-zeros element located in the y[i]-th position, 
    and = 1 ex: y = [0, 2, 1, 0], and 3 classes then return

            [[1, 0, 0, 1],
             [0, 0, 1, 0],
             [0, 1, 0, 0]]
    """</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">coo_matrix</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> 
        <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))).</span><span class="n">toarray</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Y</span> 

<span class="n">Y</span> <span class="o">=</span> <span class="n">convert_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="-kiem-tra-dao-ham"></a></p>

<h3 id="43-kiểm-tra-đạo-hàm">4.3. Kiểm tra đạo hàm</h3>
<p>Điều cốt lõi trong cách tối ưu hàm mất mát là tính gradient. Với biểu thức toán trông <em>khá rối mắt</em> như trên, rất dễ để các bạn nhầm lẫn ở một bước nào đó. Softmax Regression vẫn là một thuật toán đơn giản, sau này các bạn sẽ thấy nhưng biểu thức phức tạp hơn nhiều. Rất khó để có thể tính toán đúng gradient ở ngay lần thử đầu tiên.</p>

<p>Trong thực nghiệm, một cách thường được làm là so sánh gradient tính được với <em>numeric gradient</em>, tức gradient tính theo định nghĩa. Bạn đọc được khuyến khích đọc cách <a href="/2017/01/12/gradientdescent/#kiem-tra-dao-ham">Kiểm tra đạo hàm</a>.</p>

<p>Việc kiểm tra đạo hàm được thực hiện như sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># cost or loss function  
</span><span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">Y</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

<span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">((</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
    <span class="n">E</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">Y</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">E</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">numerical_grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">cost</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">W_p</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">W_n</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">W_p</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">eps</span> 
            <span class="n">W_n</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">eps</span>
            <span class="n">g</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W_p</span><span class="p">)</span> <span class="o">-</span> <span class="n">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W_n</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span> 

<span class="n">g1</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W_init</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">numerical_grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W_init</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g1</span> <span class="o">-</span> <span class="n">g2</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2.70479295591e-10
</code></pre></div></div>

<p>Như vậy, sự khác biệt giữa hai đạo hàm là rất nhỏ. Nếu các bạn thử vài trường hợp khác nữa của <code class="language-plaintext highlighter-rouge">N, C, d</code>, chúng ta sẽ thấy sự sai khác vẫn là nhỏ. Điều này chứng tỏ đạo hàm chúng ta tính được coi là chính xác. (Vẫn có thể có bug, chỉ khi nào kết quả cuối cùng với dữ liệu thật là chấp nhận được thì ta mới có thể bỏ cụm từ ‘có thể coi’ đi).</p>

<p>Chú ý rằng, nếu <code class="language-plaintext highlighter-rouge">N, C, d</code> quá lớn, việc tính toán <code class="language-plaintext highlighter-rouge">numerical_grad</code> trở nên cực kỳ tốn thời gian và bộ nhớ. Chúng ta chỉ nên kiểm tra với những dữ liệu nhỏ.
<a name="-ham-chinh-cho-training-softmax-regression"></a></p>

<h3 id="44-hàm-chính-cho-training-softmax-regression">4.4. Hàm chính cho training Softmax Regression</h3>

<p>Sau khi đã có những hàm cần thiết và gradient được tính đúng, chúng ta có thể viết hàm chính có training Softmax Regression (theo SGD) như sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W_init</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">max_count</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="p">[</span><span class="n">W_init</span><span class="p">]</span>    
    <span class="n">C</span> <span class="o">=</span> <span class="n">W_init</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">convert_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">check_w_after</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="k">while</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">max_count</span><span class="p">:</span>
        <span class="c1"># mix data 
</span>        <span class="n">mix_id</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">mix_id</span><span class="p">:</span>
            <span class="n">xi</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">yi</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">ai</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">xi</span><span class="p">))</span>
            <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="n">xi</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">yi</span> <span class="o">-</span> <span class="n">ai</span><span class="p">).</span><span class="n">T</span><span class="p">)</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># stopping criteria
</span>            <span class="k">if</span> <span class="n">count</span><span class="o">%</span><span class="n">check_w_after</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>                
                <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">W_new</span> <span class="o">-</span> <span class="n">W</span><span class="p">[</span><span class="o">-</span><span class="n">check_w_after</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">W</span>
            <span class="n">W</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">W_new</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W</span>
<span class="n">eta</span> <span class="o">=</span> <span class="p">.</span><span class="mi">05</span> 
<span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">softmax_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W_init</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="c1"># W[-1] is the solution, W is all history of weights
</span></code></pre></div></div>

<p><a name="-ham-du-doan-class-cho-du-lieu-moi"></a></p>

<h3 id="45-hàm-dự-đoán-class-cho-dữ-liệu-mới">4.5. Hàm dự đoán class cho dữ liệu mới</h3>

<p>Sau khi train Softmax Regression và tính được ma trận hệ số <code class="language-plaintext highlighter-rouge">W</code>, class của một dữ liệu mới có thể tìm được bằng cách xác định vị trí của giá trị lớn nhất ở đầu ra dự đoán (tương ứng với xác suất điểm dữ liệu rơi vào class đó là lớn nhất). Chú ý rằng, các class được đánh số là <code class="language-plaintext highlighter-rouge">0, 1, 2, ..., C</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="s">"""
    predict output of each columns of X
    Class of each x_i is determined by location of max probability
    Note that class are indexed by [0, 1, 2, ...., C-1]
    """</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">softmax_stable</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p><a name="-vi-du-voi-python"></a></p>

<h2 id="5-ví-dụ-với-python">5. Ví dụ với Python</h2>
<p><a name="-simulated-data"></a></p>

<h3 id="51-simulated-data">5.1. Simulated data</h3>
<p>Để minh họa cách áp dụng Softmax Regression, tôi tiếp tục làm trên <em>simulated data</em>.</p>

<p><strong>Tạo ba cụm dữ liệu</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="c1"># each column is a datapoint
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">).</span><span class="n">T</span> 
<span class="c1"># extended data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">N</span><span class="p">)),</span> <span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">original_label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">N</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">).</span><span class="n">T</span>
</code></pre></div></div>
<p>Phân bố của các dữ liệu được cho như hình dưới:</p>

<div class="imgcap">
<img src="\assets\13_softmax\ex1_1.png" align="center" width="500" />
<div class="thecap">Hình 5: Phân bố dữ liệu của các class.</div>
</div>

<p><strong>Thực hiện Softmax Regression</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">softmax_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">original_label</span><span class="p">,</span> <span class="n">W_init</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 8.45809734 -3.88415491 -3.44660294]
 [-1.11205751  1.50441603 -0.76358758]
 [ 0.24484886  0.26085383  3.3658872 ]]
</code></pre></div></div>

<p><strong>Kết quả thu được</strong></p>

<div class="imgcap">
<img src="\assets\13_softmax\ex1_2.png" align="center" width="500" />
<div class="thecap">Hình 6: Ranh giới giữa các class tìm được bằng Softmax Regression. </div>
</div>

<p>Ta thấy rằng Softmax Regression đã tạo ra các vùng cho mỗi class. Kết quả này là chấp nhận được. Từ hình trên ta cũng thấy rằng <em>đường ranh giới</em> giữa các classes là đường thẳng. Tôi sẽ chứng minh điều này ở phần sau.</p>

<p><a name="-softmax-regression-cho-mnist"></a></p>

<h3 id="52-softmax-regression-cho-mnist">5.2. Softmax Regression cho MNIST</h3>
<p>Các ví dụ trên đây được trình bày để giúp bạn đọc hiểu rõ Softmax Regression hoạt động như thế nào. Khi làm việc với các bài toán thực tế, chúng ta nên sử dụng các thư viện có sẵn, trừ khi bạn có thêm bớt vài số hạng nữa trong hàm mất mat.</p>

<p>Softmax Regression cũng được tích hợp trong hàm <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn.linear_model.LogisticRegression</a> của thư viện <a href="http://scikit-learn.org/stable/index.html">sklearn</a>.</p>

<p>Để sử dụng Softmax Regression, ta cần thêm một vài thuộc tính nữa:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">linear_model</span><span class="p">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span> <span class="n">solver</span> <span class="o">=</span> <span class="s">'lbfgs'</span><span class="p">,</span> <span class="n">multi_class</span> <span class="o">=</span> <span class="s">'multinomial'</span><span class="p">)</span>
</code></pre></div></div>

<p>Với Logistic Regression, <code class="language-plaintext highlighter-rouge">multi_class = 'ovr'</code> là giá trị mặc định, tương ứng với <strong>one-vs-rest</strong>. <code class="language-plaintext highlighter-rouge">solver = 'lbfgs'</code> là một phương pháp tối ưu cũng dựa trên gradient nhưng hiệu quả hơn và phức tạp hơn Gradient Descent. Bạn đọc có thể <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">đọc thêm ở đây</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %reset
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">from</span> <span class="nn">mnist</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">mntrain</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">'../MNIST/'</span><span class="p">)</span>
<span class="n">mntrain</span><span class="p">.</span><span class="n">load_training</span><span class="p">()</span>
<span class="n">Xtrain</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mntrain</span><span class="p">.</span><span class="n">train_images</span><span class="p">)</span><span class="o">/</span><span class="mf">255.0</span>
<span class="n">ytrain</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">mntrain</span><span class="p">.</span><span class="n">train_labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">mntest</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s">'../MNIST/'</span><span class="p">)</span>
<span class="n">mntest</span><span class="p">.</span><span class="n">load_testing</span><span class="p">()</span>
<span class="n">Xtest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mntest</span><span class="p">.</span><span class="n">test_images</span><span class="p">)</span><span class="o">/</span><span class="mf">255.0</span>
<span class="n">ytest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">mntest</span><span class="p">.</span><span class="n">test_labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1"># train
</span><span class="n">logreg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span> 
        <span class="n">solver</span> <span class="o">=</span> <span class="s">'lbfgs'</span><span class="p">,</span> <span class="n">multi_class</span> <span class="o">=</span> <span class="s">'multinomial'</span><span class="p">)</span>
<span class="n">logreg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>

<span class="c1"># test
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">logreg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"Accuracy: %.2f %%"</span> <span class="o">%</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">.</span><span class="n">tolist</span><span class="p">()))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 92.59 %
</code></pre></div></div>

<p>So với kết quả hơn 91% của one-vs-rest Logistic Regression thì Softmax Regression đã cải thiện được một chút. Kết quả thấp như thế này là có thể dự đoán được vì thực ra Softmax Regression vẫn chỉ tạo ra các đường biên là các đường tuyến tính (phẳng).</p>

<p><a name="-thao-luan"></a></p>

<h2 id="6-thảo-luận">6. Thảo luận</h2>
<p><a name="-boundary-tao-boi-softmax-regression-la-linear"></a></p>

<h3 id="61-boundary-tạo-bởi-softmax-regression-là-linear">6.1 Boundary tạo bởi Softmax Regression là linear</h3>
<p>Thật vậy, dựa vào hàm softmax thì một điểm dữ liệu \(\mathbf{x}\) được dự đoán là rơi vào class \(j\) nếu \(a_{j} \geq a_{k}, ~\forall k \neq j\). Bạn đọc có thể chứng minh được rằng \(a_{j} \geq a_{k} \Leftrightarrow z_{j} \geq z_{k}\), hay nói cách khác: 
\[
\mathbf{w}_j^T \mathbf{x} \geq \mathbf{w}_k^T\mathbf{x} \newline
\Leftrightarrow (\mathbf{w}_j - \mathbf{w}_k)^T\mathbf{x} \geq 0
\]
Đây chính là một biểu thức tuyến tính. Vậy boundary tạo bởi Softmax Regression có dạng tuyến tính. (Xem thêm <a href="/2017/01/27/logisticregression/#boundary-tao-boi-logistic-regression-co-dang-tuyen-tinh">boundary tạo bởi Logistic Regression</a>)</p>

<p><a name="-softmax-regression-la-mot-trong-hai-classifiers-pho-bien-nhat"></a></p>

<h3 id="62-softmax-regression-là-một-trong-hai-classifiers-phổ-biến-nhất">6.2. Softmax Regression là một trong hai classifiers phổ biến nhất</h3>
<p>Softmax Regression cùng với Support Vector Machine (tôi sẽ trình bày sau vài bài nữa) là hai classifier phổ biến nhất được dùng hiện nay. Softmax Regression đặc biệt được sử dụng nhiều trong các mạng Neural có nhiều lớp (Deep Neural Networks hay DNN). Những lớp phía trước có thể được coi như một bộ <a href="/general/2017/02/06/featureengineering/#feature-extractor">Feature Extractor</a>, lớp cuối cùng của DNN cho bài toán classification thường là Softmax Regression.</p>

<p><a name="-source-code"></a></p>

<h3 id="63-source-code">6.3. Source code</h3>

<p>Các bạn có thể tìm thấy source code trong <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/13_softmax/Softmax%20Regression.ipynb">jupyter notebook này</a>.
<a name="tai-lieu-tham-khao"></a></p>

<h2 id="tài-liệu-tham-khảo">Tài liệu tham khảo</h2>
<p>[1] <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/">Softmax Regression</a></p>

<p>[2] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn.linear_model.LogisticRegression</a></p>

<p>[3] <a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax function - Wikipedia</a></p>

<p>[4] <a href="http://neuralnetworksanddeeplearning.com/chap3.html">Improving the way neural networks learn</a></p>

:ET